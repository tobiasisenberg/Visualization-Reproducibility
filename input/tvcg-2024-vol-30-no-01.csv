"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Affective Visualization Design: Leveraging the Emotional Impact of Data","X. Lan; Y. Wu; N. Cao","Fudan University, Research Group of Computational and AI Communication at Institute for Global Communications and Integrated Media, China; Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1","11","In recent years, more and more researchers have reflected on the undervaluation of emotion in data visualization and highlighted the importance of considering human emotion in visualization design. Meanwhile, an increasing number of studies have been conducted to explore emotion-related factors. However, so far, this research area is still in its early stages and faces a set of challenges, such as the unclear definition of key concepts, the insufficient justification of why emotion is important in visualization design, and the lack of characterization of the design space of affective visualization design. To address these challenges, first, we conducted a literature review and identified three research lines that examined both emotion and data visualization. We clarified the differences between these research lines and kept 109 papers that studied or discussed how data visualization communicates and influences emotion. Then, we coded the 109 papers in terms of how they justified the legitimacy of considering emotion in visualization design (i.e., why emotion is important) and identified five argumentative perspectives. Based on these papers, we also identified 61 projects that practiced affective visualization design. We coded these design projects in three dimensions, including design fields (where), design tasks (what), and design methods (how), to explore the design space of affective visualization design.","1941-0506","","10.1109/TVCG.2023.3327385","NSFC(grant numbers:62072338); NSF(grant numbers:20ZR1461500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301796","Information Visualization;Affective Design;Visual Communication;User Experience;Storytelling","Data visualization;Image color analysis;Task analysis;Systematics;Faces;Visual analytics;Space exploration","Humans;Computer Graphics;Emotions","","","96","IEEE","30 Oct 2023","","","IEEE","IEEE Journals"
"Fast Compressed Segmentation Volumes for Scientific Visualization","M. Piochowiak; C. Dachsbacher","Karlsruhe Institute of Technology, Germany; Karlsruhe Institute of Technology, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","12","22","Voxel-based segmentation volumes often store a large number of labels and voxels, and the resulting amount of data can make storage, transfer, and interactive visualization difficult. We present a lossless compression technique which addresses these challenges. It processes individual small bricks of a segmentation volume and compactly encodes the labelled regions and their boundaries by an iterative refinement scheme. The result for each brick is a list of labels, and a sequence of operations to reconstruct the brick which is further compressed using rANS-entropy coding. As the relative frequencies of operations are very similar across bricks, the entropy coding can use global frequency tables for an entire data set which enables efficient and effective parallel (de)compression. Our technique achieves high throughput (up to gigabytes per second both for compression and decompression) and strong compression ratios of about 1% to 3% of the original data set size while being applicable to GPU-based rendering. We evaluate our method for various data sets from different fields and demonstrate GPU-based volume visualization with on-the-fly decompression, level-of-detail rendering (with optional on-demand streaming of detail coefficients to the GPU), and a caching strategy for decompressed bricks for further performance improvement.","1941-0506","","10.1109/TVCG.2023.3326573","Helmholtz Association (HGF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292793","Segmentation volumes;lossless compression;volume rendering","Rendering (computer graphics);Data visualization;Image coding;Decoding;Octrees;Image segmentation;Graphics processing units","","","","50","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Swaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms","F. Yang; M. Cai; C. Mortenson; H. Fakhari; A. D. Lokmanoglu; J. Hullman; S. Franconeri; N. Diakopoulos; E. C. Nisbet; M. Kay","Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","23","33","We conducted a longitudinal study during the 2022 U.S. midterm elections, investigating the real-world impacts of uncertainty visualizations. Using our forecast model of the governor elections in 33 states, we created a website and deployed four uncertainty visualizations for the election forecasts: single quantile dotplot (1-Dotplot), dual quantile dotplots (2-Dotplot), dual histogram intervals (2-Interval), and Plinko quantile dotplot (Plinko), an animated design with a physical and probabilistic analogy. Our online experiment ran from Oct. 18, 2022, to Nov. 23, 2022, involving 1,327 participants from 15 states. We use Bayesian multilevel modeling and post-stratification to produce demographically-representative estimates of people's emotions, trust in forecasts, and political participation intention. We find that election forecast visualizations can heighten emotions, increase trust, and slightly affect people's intentions to participate in elections. 2-Interval shows the strongest effects across all measures; 1-Dotplot increases trust the most after elections. Both visualizations create emotional and trust gaps between different partisan identities, especially when a Republican candidate is predicted to win. Our qualitative analysis uncovers the complex political and social contexts of election forecast visualizations, showcasing that visualizations may provoke polarization. This intriguing interplay between visualization types, partisanship, and trust exemplifies the fundamental challenge of disentangling visualization from its context, underscoring a need for deeper investigation into the real-world impacts of visualizations. Our preprint and supplements are available at https://doi.org/osf.io/ajq8f.","1941-0506","","10.1109/TVCG.2023.3327356","NSF(grant numbers:IIS-2107490,IIS-1901485,IIS-2126598,2127309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309864","Uncertainty visualization;Probabilistic forecasts;Elections;Emotions;Trust;Political participation;Longitudinal study","Voting;Visualization;Uncertainty;Predictive models;Probabilistic logic;Sociology;Journalism","Humans;Bayes Theorem;Computer Graphics;Emotions;Intention;Longitudinal Studies;Trust;Politics;Forecasting","","","92","IEEE","6 Nov 2023","","","IEEE","IEEE Journals"
"TimeSplines: Sketch-Based Authoring of Flexible and Idiosyncratic Timelines","A. Offenwanger; M. Brehmer; F. Chevalier; T. Tsandilas","Université Paris Saclay, CRNS, Inria, LISN, France; Tableau Research, USA; Departments of Computer Science and Statistical Sciences, University of Toronto, Canada; Université Paris Saclay, CRNS, Inria, LISN, France","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","34","44","Timelines are essential for visually communicating chronological narratives and reflecting on the personal and cultural significance of historical events. Existing visualization tools tend to support conventional linear representations, but fail to capture personal idiosyncratic conceptualizations of time. In response, we built TimeSplines, a visualization authoring tool that allows people to sketch multiple free-form temporal axes and populate them with heterogeneous, time-oriented data via incremental and lazy data binding. Authors can bend, compress, and expand temporal axes to emphasize or de-emphasize intervals based on their personal importance; they can also annotate the axes with text and figurative elements to convey contextual information. The results of two user studies show how people appropriate the concepts in TimeSplines to express their own conceptualization of time, while our curated gallery of images demonstrates the expressive potential of our approach.","1941-0506","","10.1109/TVCG.2023.3326520","CNRS; University of Toronto; French National Research Agency(grant numbers:ANR-21-CE33-0002 GLACIS); NSERC(grant numbers:RGPIN-2018-05072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308646","Temporal Data;interaction design;communication / presentation;storytelling;sketch-based interface;lazy data binding","Data visualization;Shape;Annotations;Visualization;Spirals;Reflection;History","","","","79","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"Visualization of Discontinuous Vector Field Topology","E. Miftari; D. Durstewitz; F. Sadlo","Heidelberg University, Germany; Heidelberg University, Germany; Heidelberg University, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","45","54","This paper extends the concept and the visualization of vector field topology to vector fields with discontinuities. We address the non-uniqueness of flow in such fields by introduction of a time-reversible concept of equivalence. This concept generalizes streamlines to streamsets and thus vector field topology to discontinuous vector fields in terms of invariant streamsets. We identify respective novel critical structures as well as their manifolds, investigate their interplay with traditional vector field topology, and detail the application and interpretation of our approach using specifically designed synthetic cases and a simulated case from physics.","1941-0506","","10.1109/TVCG.2023.3326519","Deutsche Forschungsgemeinschaft (DFG)(grant numbers:390900948,281071066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296524","Discontinuous vector field topology;equivalence in non-unique flow;non-smooth dynamical systems","Topology;Manifolds;Eigenvalues and eigenfunctions;Dynamical systems;Switches;Orbits;Behavioral sciences","","","","23","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Vortex Lens: Interactive Vortex Core Line Extraction using Observed Line Integral Convolution","P. Rautek; X. Zhang; B. Woschizka; T. Theußl; M. Hadwiger","King Abdullah University of Science and Technology (KAUST), Visual Computing Center, Thuwal, Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Visual Computing Center, Thuwal, Saudi Arabia; King Abdullah Univ. of Sci. & Technol. (KAUST), Vis. Comput. Ctr., Thuwal, Saudi Arabia; Core Labs, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Visual Computing Center, Thuwal, Saudi Arabia","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","55","65","This paper describes a novel method for detecting and visualizing vortex structures in unsteady 2D fluid flows. The method is based on an interactive local reference frame estimation that minimizes the observed time derivative of the input flow field $\mathrm{v}(x, t)$. A locally optimal reference frame $\mathrm{w}(x, t)$ assists the user in the identification of physically observable vortex structures in Observed Line Integral Convolution (LIC) visualizations. The observed LIC visualizations are interactively computed and displayed in a user-steered vortex lens region, embedded in the context of a conventional LIC visualization outside the lens. The locally optimal reference frame is then used to detect observed critical points, where $\mathrm{v}=\mathrm{w}$, which are used to seed vortex core lines. Each vortex core line is computed as a solution of the ordinary differential equation (ODE) $\dot{w}(t)=\mathrm{w}(w(t), t)$, with an observed critical point as initial condition $(w(t_{0}), t_{0})$. During integration, we enforce a strict error bound on the difference between the extracted core line and the integration of a path line of the input vector field, i.e., a solution to the ODE $\dot{v}(t)=\mathrm{v}(v(t), t)$. We experimentally verify that this error depends on the step size of the core line integration. This ensures that our method extracts Lagrangian vortex core lines that are the simultaneous solution of both ODEs with a numerical error that is controllable by the integration step size. We show the usability of our method in the context of an interactive system using a lens metaphor, and evaluate the results in comparison to state-of-the-art vortex core line extraction methods.","1941-0506","","10.1109/TVCG.2023.3326915","King Abdullah University of Science and Technology (KAUST); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294201","Flow visualization;vortex detection;objectivity;observers;reference frames;Lie algebras;visual lens metaphors","Lenses;Visualization;Observers;Optimization;Convolution;Fluids;Linear programming","","","","53","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Guided Visual Analytics for Image Selection in Time and Space","I. Pérez-Messina; D. Ceneda; S. Miksch","TU Wien, Austria; TU Wien, Austria; TU Wien, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","66","75","Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.","1941-0506","","10.1109/TVCG.2023.3326572","Vienna Science and Technology Fund (WWTF)(grant numbers:10.47379/ICT19047); Austrian Science Fund (FWF)(grant numbers:P31419-N31); FFG(grant numbers:880883); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308619","Application Motivated Visualization;Geospatial Data;Mixed Initiative Human-Machine Analysis;Process/Workflow Design;Task Abstractions & Application Domains;Temporal Data","Task analysis;Decision making;Weapons;Optimization;Metadata;Image reconstruction;Prototypes","","","","37","CCBY","3 Nov 2023","","","IEEE","IEEE Journals"
"A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision","C. Chen; Y. Guo; F. Tian; S. Liu; W. Yang; Z. Wang; J. Wu; H. Su; H. Pfister; S. Liu","School of Software, BNRist, Tsinghua University, China; School of Software, BNRist, Tsinghua University, China; School of Software, BNRist, Tsinghua University, China; Department of Computer Science and Technology, Tsinghua University, China; School of Software, BNRist, Tsinghua University, China; School of Software, BNRist, Tsinghua University, China; Cardiff University, United Kingdom; Department of Computer Science and Technology, Tsinghua University, China; Harvard University, United Kingdom; School of Software, BNRist, Tsinghua University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","76","86","Existing model evaluation tools mainly focus on evaluating classification models, leaving a gap in evaluating more complex models, such as object detection. In this paper, we develop an open-source visual analysis tool, Uni-Evaluator, to support a unified model evaluation for classification, object detection, and instance segmentation in computer vision. The key idea behind our method is to formulate both discrete and continuous predictions in different tasks as unified probability distributions. Based on these distributions, we develop 1) a matrix-based visualization to provide an overview of model performance; 2) a table visualization to identify the problematic data subsets where the model performs poorly; 3) a grid visualization to display the samples of interest. These visualizations work together to facilitate the model evaluation from a global overview to individual samples. Two case studies demonstrate the effectiveness of Uni-Evaluator in evaluating model performance and making informed improvements.","1941-0506","","10.1109/TVCG.2023.3326588","National Natural Science Foundation of China(grant numbers:U21A20469,61936002,92248303); National Key R&D Program of China(grant numbers:2020YFB2104100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297566","Model evaluation;computer vision;classification;object detection;instance segmentation","Computational modeling;Task analysis;Computer vision;Marine vehicles;Data visualization;Predictive models;Object detection","","","","69","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"VideoPro: A Visual Analytics Approach for Interactive Video Programming","J. He; X. Wang; K. K. Wong; X. Huang; C. Chen; Z. Chen; F. Wang; M. Zhu; H. Qu","Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Tsinghua University, Beijing, China; Hong Kong University of Science and Technology, Hong Kong, China; Sichuang University, Chengdu, China; Sichuang University, Chengdu, China; Hong Kong University of Science and Technology, Hong Kong, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","87","97","Constructing supervised machine learning models for real-world video analysis require substantial labeled data, which is costly to acquire due to scarce domain expertise and laborious manual inspection. While data programming shows promise in generating labeled data at scale with user-defined labeling functions, the high dimensional and complex temporal information in videos poses additional challenges for effectively composing and evaluating labeling functions. In this paper, we propose VideoPro, a visual analytics approach to support flexible and scalable video data programming for model steering with reduced human effort. We first extract human-understandable events from videos using computer vision techniques and treat them as atomic components of labeling functions. We further propose a two-stage template mining algorithm that characterizes the sequential patterns of these events to serve as labeling function templates for efficient data labeling. The visual interface of VideoPro facilitates multifaceted exploration, examination, and application of the labeling templates, allowing for effective programming of video data at scale. Moreover, users can monitor the impact of programming on model performance and make informed adjustments during the iterative programming process. We demonstrate the efficiency and effectiveness of our approach with two case studies and expert interviews.","1941-0506","","10.1109/TVCG.2023.3326586","ITF PRP(grant numbers:PRP/001/21FX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292616","Interactive machine learning;data programming;video exploration and analysis","Labeling;Programming;Streaming media;Data models;Visual analytics;Task analysis;Semantics","","1","","83","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Character-Oriented Design for Visual Data Storytelling","K. Dasu; Y. -H. Kuo; K. -L. Ma","University of California, Davis, USA; University of California, Davis, USA; University of California, Davis, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","98","108","When telling a data story, an author has an intention they seek to convey to an audience. This intention can be of many forms such as to persuade, to educate, to inform, or even to entertain. In addition to expressing their intention, the story plot must balance being consumable and enjoyable while preserving scientific integrity. In data stories, numerous methods have been identified for constructing and presenting a plot. However, there is an opportunity to expand how we think and create the visual elements that present the story. Stories are brought to life by characters; often they are what make a story captivating, enjoyable, memorable, and facilitate following the plot until the end. Through the analysis of 160 existing data stories, we systematically investigate and identify distinguishable features of characters in data stories, and we illustrate how they feed into the broader concept of “character-oriented design”. We identify the roles and visual representations data characters assume as well as the types of relationships these roles have with one another. We identify characteristics of antagonists as well as define conflict in data stories. We find the need for an identifiable central character that the audience latches on to in order to follow the narrative and identify their visual representations. We then illustrate “character-oriented design” by showing how to develop data characters with common data story plots. With this work, we present a framework for data characters derived from our analysis; we then offer our extension to the data storytelling process using character-oriented design. To access our supplemental materials please visit https://chaorientdesignds.github.io/.","1941-0506","","10.1109/TVCG.2023.3326578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290935","Storytelling;Explanatory;Narrative visualization;Visual metaphor","Visualization;Data visualization;Organizations;Terminology;Task analysis;Media;Stars","","","","74","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Data Player: Automatic Generation of Data Videos with Narration-Animation Interplay","L. Shen; Y. Zhang; H. Zhang; Y. Wang","The Hong Kong University of Science and Technology, China; Cornell University, USA; Microsoft Research Asia (MSRA), China; Microsoft Research Asia (MSRA), China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","109","119","Data visualizations and narratives are often integrated to convey data stories effectively. Among various data storytelling formats, data videos have been garnering increasing attention. These videos provide an intuitive interpretation of data charts while vividly articulating the underlying data insights. However, the production of data videos demands a diverse set of professional skills and considerable manual labor, including understanding narratives, linking visual elements with narration segments, designing and crafting animations, recording audio narrations, and synchronizing audio with visual animations. To simplify this process, our paper introduces a novel method, referred to as Data Player, capable of automatically generating dynamic data videos with narration-animation interplay. This approach lowers the technical barriers associated with creating data videos rich in narration. To enable narration-animation interplay, Data Player constructs references between visualizations and text input. Specifically, it first extracts data into tables from the visualizations. Subsequently, it utilizes large language models to form semantic connections between text and visuals. Finally, Data Player encodes animation design knowledge as computational low-level constraints, allowing for the recommendation of suitable animation presets that align with the audio narration produced by text-to-speech technologies. We assessed Data Player's efficacy through an example gallery, a user study, and expert interviews. The evaluation results demonstrated that Data Player can generate high-quality data videos that are comparable to human-composed ones.","1941-0506","","10.1109/TVCG.2023.3327197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308700","Visualization;Narration-animation interplay;Data video;Human-AI collaboration","Videos;Data visualization;Animation;Visualization;Interviews;Electronic mail;Data models","","","","68","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"EMPHASISCHECKER: A Tool for Guiding Chart and Caption Emphasis","D. H. Kim; S. Choi; J. Kim; V. Setlur; M. Agrawala","KAIST, South Korea; KAIST, South Korea; KAIST, South Korea; Tableau Research, USA; Stanford University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","120","130","Recent work has shown that when both the chart and caption emphasize the same aspects of the data, readers tend to remember the doubly-emphasized features as takeaways; when there is a mismatch, readers rely on the chart to form takeaways and can miss information in the caption text. Through a survey of 280 chart-caption pairs in real-world sources (e.g., news media, poll reports, government reports, academic articles, and Tableau Public), we find that captions often do not emphasize the same information in practice, which could limit how effectively readers take away the authors' intended messages. Motivated by the survey findings, we present EMPHASISCHECKER, an interactive tool that highlights visually prominent chart features as well as the features emphasized by the caption text along with any mismatches in the emphasis. The tool implements a time-series prominent feature detector based on the Ramer-Douglas-Peucker algorithm and a text reference extractor that identifies time references and data descriptions in the caption and matches them with chart data. This information enables authors to compare features emphasized by these two modalities, quickly see mismatches, and make necessary revisions. A user study confirms that our tool is both useful and easy to use when authoring charts and captions.","1941-0506","","10.1109/TVCG.2023.3327150","Brown Institute for Media Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308624","Chart and text takeaways;visual prominence;authoring;captions","Data visualization;Feature extraction;Electronic mail;Surveys;Bars;Detectors;Data mining","","","","81","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback","G. Wu; S. Guo; J. Hoffswell; G. Y. -Y. Chan; R. A. Rossi; E. Koh","New York University, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA; Adobe Research, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","131","141","Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.","1941-0506","","10.1109/TVCG.2023.3327363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308697","Narrative visualization;visual storytelling;conversational agent","Data visualization;Visualization;Videos;Prototypes;Interviews;Optimization;Authoring systems","","","","79","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"A Parallel Framework for Streaming Dimensionality Reduction","J. Xia; L. Huang; Y. Sun; Z. Deng; X. L. Zhang; M. Zhu","School of Computer Science and Engineering, Central South University, China; School of Computer Science and Engineering, Central South University, China; School of Computer Science and Engineering, Central South University, China; School of Computer Science and Engineering, Central South University, China; Pennsylvania State University, USA; Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","142","152","The visualization of streaming high-dimensional data often needs to consider the speed in dimensionality reduction algorithms, the quality of visualized data patterns, and the stability of view graphs that usually change over time with new data. Existing methods of streaming high-dimensional data visualization primarily line up essential modules in a serial manner and often face challenges in satisfying all these design considerations. In this research, we propose a novel parallel framework for streaming high-dimensional data visualization to achieve high data processing speed, high quality in data patterns, and good stability in visual presentations. This framework arranges all essential modules in parallel to mitigate the delays caused by module waiting in serial setups. In addition, to facilitate the parallel pipeline, we redesign these modules with a parametric non-linear embedding method for new data embedding, an incremental learning method for online embedding function updating, and a hybrid strategy for optimized embedding updating. We also improve the coordination mechanism among these modules. Our experiments show that our method has advantages in embedding speed, quality, and stability over other existing methods to visualize streaming high-dimensional data.","1941-0506","","10.1109/TVCG.2023.3326515","National Natural Science Foundation of China(grant numbers:61872389); Foundation for Excellent Youth of Hunan Province(grant numbers:2023JJ10080); Central South University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292610","High-dimensional data visualization;dimensionality reduction;streaming data visualization","Data visualization;Stability analysis;Heuristic algorithms;Power system stability;Task analysis;Principal component analysis;Pipelines","","","","77","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"QEVIS: Multi-Grained Visualization of Distributed Query Execution","Q. Shen; Z. You; X. Yan; C. Zhang; K. Xu; D. Zeng; J. Qin; B. Tang","Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China; Huawei Technologies Co., Ltd., China; Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, China; Shenzhen Institute of Computing Sciences, Shenzhen University, China; Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","153","163","Distributed query processing systems such as Apache Hive and Spark are widely-used in many organizations for large-scale data analytics. Analyzing and understanding the query execution process of these systems are daily routines for engineers and crucial for identifying performance problems, optimizing system configurations, and rectifying errors. However, existing visualization tools for distributed query execution are insufficient because (i) most of them (if not all) do not provide fine-grained visualization (i.e., the atomic task level), which can be crucial for understanding query performance and reasoning about the underlying execution anomalies, and (ii) they do not support proper linkages between system status and query execution, which makes it difficult to identify the causes of execution problems. To tackle these limitations, we propose QEVIS, which visualizes distributed query execution process with multiple views that focus on different granularities and complement each other. Specifically, we first devise a query logical plan layout algorithm to visualize the overall query execution progress compactly and clearly. We then propose two novel scoring methods to summarize the anomaly degrees of the jobs and machines during query execution, and visualize the anomaly scores intuitively, which allow users to easily identify the components that are worth paying attention to. Moreover, we devise a scatter plot-based task view to show a massive number of atomic tasks, where task distribution patterns are informative for execution problems. We also equip QEVIS with a suite of auxiliary views and interaction methods to support easy and effective cross-view exploration, which makes it convenient to track the causes of execution problems. QEVIS has been used in the production environment of our industry partner, and we present three use cases from real-world applications and user interview to demonstrate its effectiveness. QEVIS is open-source at https://github.com/DBGroup-SUSTech/QEVIS.","1941-0506","","10.1109/TVCG.2023.3326930","National Key R&D program of China(grant numbers:2021YFB3301500); Guangdong Provincial Natural Science Foundation(grant numbers:2019A1515111047); Shenzhen Colleges and Universities Stable Support Grant(grant numbers:20200811104054002); Shenzhen Fundamental Research Program(grant numbers:20220815112848002); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); Huawei Guass department; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297121","visual analytics system;distributed query execution;performance analysis","Task analysis;Data visualization;Sparks;Layout;Engines;Couplings;Visual analytics","","","","55","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Class-Constrained t-SNE: Combining Data Features and Class Probabilities","L. Meng; S. van den Elzen; N. Pezzotti; A. Vilanova","Eindhoven University of Technology, Netherlands; Eindhoven University of Technology, Netherlands; Eindhoven University of Technology, Netherlands; Eindhoven University of Technology, Netherlands","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","164","174","Data features and class probabilities are two main perspectives when, e.g., evaluating model results and identifying problematic items. Class probabilities represent the likelihood that each instance belongs to a particular class, which can be produced by probabilistic classifiers or even human labeling with uncertainty. Since both perspectives are multi-dimensional data, dimensionality reduction (DR) techniques are commonly used to extract informative characteristics from them. However, existing methods either focus solely on the data feature perspective or rely on class probability estimates to guide the DR process. In contrast to previous work where separate views are linked to conduct the analysis, we propose a novel approach, class-constrained t-SNE, that combines data features and class probabilities in the same DR result. Specifically, we combine them by balancing two corresponding components in a cost function to optimize the positions of data points and iconic representation of classes – class landmarks. Furthermore, an interactive user-adjustable parameter balances these two components so that users can focus on the weighted perspectives of interest and also empowers a smooth visual transition between varying perspectives to preserve the mental map. We illustrate its application potential in model evaluation and visual-interactive labeling. A comparative analysis is performed to evaluate the DR results.","1941-0506","","10.1109/TVCG.2023.3326600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294259","Dimensionality reduction;t-distributed stochastic neighbor embedding;constraint integration","Data visualization;Data models;Visualization;Analytical models;Labeling;Cost function;Periodic structures","","","","60","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"ManiVault: A Flexible and Extensible Visual Analytics Framework for High-Dimensional Data","A. Vieth; T. Kroes; J. Thijssen; B. van Lew; J. Eggermont; S. Basu; E. Eisemann; A. Vilanova; T. Höllt; B. Lelieveldt","TU Delft, Netherlands; Leiden University Medical Center, Netherlands; Leiden University Medical Center, Netherlands; Leiden University Medical Center, Netherlands; Leiden University Medical Center, Netherlands; Leiden University Medical Center, Netherlands; TU Delft, Netherlands; TU Eindhoven, Netherlands; TU Delft, Netherlands; TU Delft, Netherlands","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","175","185","Exploration and analysis of high-dimensional data are important tasks in many fields that produce large and complex data, like the financial sector, systems biology, or cultural heritage. Tailor-made visual analytics software is developed for each specific application, limiting their applicability in other fields. However, as diverse as these fields are, their characteristics and requirements for data analysis are conceptually similar. Many applications share abstract tasks and data types and are often constructed with similar building blocks. Developing such applications, even when based mostly on existing building blocks, requires significant engineering efforts. We developed ManiVault, a flexible and extensible open-source visual analytics framework for analyzing high-dimensional data. The primary objective of ManiVault is to facilitate rapid prototyping of visual analytics workflows for visualization software developers and practitioners alike. ManiVault is built using a plugin-based architecture that offers easy extensibility. While our architecture deliberately keeps plugins self-contained, to guarantee maximum flexibility and re-usability, we have designed and implemented a messaging API for tight integration and linking of modules to support common visual analytics design patterns. We provide several visualization and analytics plugins, and ManiVault's API makes the integration of new plugins easy for developers. ManiVault facilitates the distribution of visualization and analysis pipelines and results for practitioners through saving and reproducing complete application states. As such, ManiVault can be used as a communication tool among researchers to discuss workflows and results. A copy of this paper and all supplemental material is available at osf.io/9k6jw, and source code at github.com/ManiVaultStudio.","1941-0506","","10.1109/TVCG.2023.3326582","NWO(grant numbers:17126,024.004.012); NIH(grant numbers:UM1MH130981); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290921","High-dimensional data;Visual analytics;Visualization framework;Progressive analytics;Prototyping system","Data visualization;Visual analytics;Software;Graphical user interfaces;Task analysis;Spatial databases;Python","","","","73","CCBY","23 Oct 2023","","","IEEE","IEEE Journals"
"Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting","M. N. Hoque; N. Elmqvist","University of Maryland, College Park, College Park, MD, USA; Aarhus University, Aarhus, Denmark","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","186","196","We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a “born scalable” query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as $\mathbb{P}^{6}$: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.","1941-0506","","10.1109/TVCG.2023.3326594","U.S. National Science Foundation(grant numbers:2211628); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292608","Multidimensional data visualization;multivariate graphs;visual queries;visual exploration","Visualization;Fluids;Aggregates;Data visualization;Prototypes;Data aggregation;Mobile handsets","","","","54","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Dead or Alive: Continuous Data Profiling for Interactive Data Science","W. Epperson; V. Gorantla; D. Moritz; A. Perer","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","197","207","Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively (“live”) and the other updates only on demand (“dead”). We find that both tools, dead or alive, facilitate insight discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support.","1941-0506","","10.1109/TVCG.2023.3327367","Brookhaven National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301695","Data Profiling;Data Quality;Exploratory Data Analysis;Interactive Data Science","Codes;Data visualization;Data models;Visualization;Programming;Manuals;Data science","","","","51","CCBYNCND","30 Oct 2023","","","IEEE","IEEE Journals"
"EVM: Incorporating Model Checking into Exploratory Visual Analysis","A. Kale; Z. Guo; X. L. Qiao; J. Heer; J. Hullman","University of Chicago, USA; Northwestern University, USA; Northwestern University, USA; University of Washington, USA; Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","208","218","Visual analytics (VA) tools support data exploration by helping analysts quickly and iteratively generate views of data which reveal interesting patterns. However, these tools seldom enable explicit checks of the resulting interpretations of data—e.g., whether patterns can be accounted for by a model that implies a particular structure in the relationships between variables. We present EVM, a data exploration tool that enables users to express and check provisional interpretations of data in the form of statistical models. EVM integrates support for visualization-based model checks by rendering distributions of model predictions alongside user-generated views of data. In a user study with data scientists practicing in the private and public sector, we evaluate how model checks influence analysts' thinking during data exploration. Our analysis characterizes how participants use model checks to scrutinize expectations about data generating process and surfaces further opportunities to scaffold model exploration in VA tools.","1941-0506","","10.1109/TVCG.2023.3326516","NSF(grant numbers:2211939,1930642); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291034","Visualization;model checks;exploratory analysis","Data models;Analytical models;Visualization;Data visualization;Predictive models;Model checking;Computational modeling","","1","","70","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions","X. Teng; Y. Ahn; Y. -R. Lin","University of Pittsburgh, USA; University of Pittsburgh, USA; University of Pittsburgh, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","219","229","Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of causality. Additionally, we propose a REASONING STORYBOARD, which uses a flow-based approach to illustrate paradoxical phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure accountable decision-making. Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed “de-paradox” workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.","1941-0506","","10.1109/TVCG.2023.3326587","AFOSR; DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292663","Causal Analysis;Simpson's Paradox;Spurious Associations;Machine Learning;Decision Making","Visual analytics;Decision making;Training;Interviews;Cognition;Machine learning;Systematics","","","","76","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Visualization According to Statisticians: An Interview Study on the Role of Visualization for Inferential Statistics","E. Newburger; N. Elmqvist","U.S. Naval Academy, Annapolis, MD, USA; Aarhus University, Aarhus, Denmark","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","230","239","Statisticians are not only one of the earliest professional adopters of data visualization, but also some of its most prolific users. Understanding how these professionals utilize visual representations in their analytic process may shed light on best practices for visual sensemaking. We present results from an interview study involving 18 professional statisticians (19.7 years average in the profession) on three aspects: (1) their use of visualization in their daily analytic work; (2) their mental models of inferential statistical processes; and (3) their design recommendations for how to best represent statistical inferences. Interview sessions consisted of discussing inferential statistics, eliciting participant sketches of suitable visual designs, and finally, a design intervention with our proposed visual designs. We analyzed interview transcripts using thematic analysis and open coding, deriving thematic codes on statistical mindset, analytic process, and analytic toolkit. The key findings for each aspect are as follows: (1) statisticians make extensive use of visualization during all phases of their work (and not just when reporting results); (2) their mental models of inferential methods tend to be mostly visually based; and (3) many statisticians abhor dichotomous thinking. The latter suggests that a multi-faceted visual display of inferential statistics that includes a visual indicator of analytically important effect sizes may help to balance the attributed epistemic power of traditional statistical testing with an awareness of the uncertainty of sensemaking.","1941-0506","","10.1109/TVCG.2023.3326521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290952","Inferential statistics;qualitative interview study;thematic coding;statistical visualization","Data visualization;Interviews;Visualization;Industries;Encoding;Cognitive science;Codes","","","","32","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Cluster-Aware Grid Layout","Y. Zhou; W. Yang; J. Chen; C. Chen; Z. Shen; X. Luo; L. Yu; S. Liu","School of Software, BNRist, Tsinghua University, China; School of Software, BNRist, Tsinghua University, China; School of Software, BNRist, Tsinghua University, China; Kuaishou Technology, China; School of Software, BNRist, Tsinghua University, China; Guilin University of Electronic Technology, China; Xi'an Jiaotong-Liverpool University, China; School of Software, BNRist, Tsinghua University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","240","250","Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.","1941-0506","","10.1109/TVCG.2023.3326934","National Natural Science Foundation of China(grant numbers:U21A20469,61936002); National Key R&D Program of China(grant numbers:2020YFB2104100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292929","Grid layout;similarity;convexity;compactness;optimization","Area measurement;Shape;Layout;Shape measurement;Particle measurements;Atmospheric measurements;Correlation","","","","61","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Radial Icicle Tree (RIT): Node Separation and Area Constancy","Y. Jin; T. J. A. de Jong; M. Tennekes; M. Chen","University of Oxford, UK; Statistics Netherlands, the Netherlands; Statistics Netherlands, the Netherlands; University of Oxford, UK","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","251","261","Icicles and sunbursts are two commonly-used visual representations of trees. While icicle trees can map data values faithfully to rectangles of different sizes, often some rectangles are too narrow to be noticed easily. When an icicle tree is transformed into a sunburst tree, the width of each rectangle becomes the length of an annular sector that is usually longer than the original width. While sunburst trees alleviate the problem of narrow rectangles in icicle trees, it no longer maintains the consistency of size encoding. At different tree depths, nodes of the same data values are displayed in annular sections of different sizes in a sunburst tree, though they are represented by rectangles of the same size in an icicle tree. Furthermore, two nodes from different subtrees could sometimes appear as a single node in both icicle trees and sunburst trees. In this paper, we propose a new visual representation, referred to as radial icicle tree (RIT), which transforms the rectangular bounding box of an icicle tree into a circle, circular sector, or annular sector while introducing gaps between nodes and maintaining area constancy for nodes of the same size. We applied the new visual design to several datasets. Both the analytical design process and user-centered evaluation have confirmed that this new design has improved the design of icicles and sunburst trees without introducing any relative demerit.","1941-0506","","10.1109/TVCG.2023.3327178","Network of European Data Scientists (NeEDS); Research and Innovation Staff Exchange (RISE); Marie Sklodowska-Curie Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297579","Tree visualization;icicle tree;sunburst tree;size encoding;area constancy;node separation;radial icicle tree;RIT","Data visualization;Visualization;Encoding;Graphics;Distortion;Image color analysis;Decoding","","","","54","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"AttentionViz: A Global View of Transformer Attention","C. Yeh; Y. Chen; A. Wu; C. Chen; F. Viégas; M. Wattenberg","Harvard University, USA; Harvard University, USA; Harvard University, USA; Harvard University, USA; Harvard University, USA; Harvard University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","262","272","Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.","1941-0506","","10.1109/TVCG.2023.3327163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297591","Transformer;Attention;NLP;Computer Vision;Visual Analytics","Transformers;Visualization;Head;Encoding;Computational modeling;Bidirectional control;Task analysis","","1","","62","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models","X. Wang; R. Huang; Z. Jin; T. Fang; H. Qu","Weill Cornell Medical College, Cornell University, USA; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","273","283","Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models' implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models' relational reasoning over concepts in different situations.","1941-0506","","10.1109/TVCG.2023.3327153","Hong Kong Theme-based Research Scheme(grant numbers:T41-709/17N); MSRA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297594","Commonsense reasoning;visual analytics;XAI;natural language processing","Commonsense reasoning;Context modeling;Analytical models;Natural language processing;Benchmark testing;Task analysis;Data models","","","","77","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model","S. Xiao; S. Huang; Y. Lin; Y. Ye; W. Zeng","Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","284","294","Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in an engaging and informative manner. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise data integrity. Text-guided generation methods are emerging, but may have limited applicability due to their predefined entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative models. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the design practices identified from empirical research into existing pictorial visualizations. We further develop an interactive visual interface that integrates a text analyzer, editing module, and evaluation module to enable users to generate, modify, and assess pictorial visualizations. We experimentally demonstrate the usability of our tool, and conclude with a discussion of the potential of using text-to-image generative models combined with an interactive interface for visualization design.","1941-0506","","10.1109/TVCG.2023.3326913","National Natural Science Foundation of China(grant numbers:62172398); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296520","pictorial visualization;generative model;authoring tool","Visualization;Data visualization;Semantics;Authoring systems;Pipelines;Data models;Interviews","","1","","61","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation","Y. Feng; X. Wang; K. K. Wong; S. Wang; Y. Lu; M. Zhu; B. Wang; W. Chen","State Key Lab of CAD&CG, Zhejiang University, China; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","295","305","Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.","1941-0506","","10.1109/TVCG.2023.3327168","National Natural Science Foundation of China(grant numbers:62132017); Fundamental Research Funds for the Central Universities(grant numbers:226-2022-00235); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296017","Prompt engineering;text-to-image generation;image visualization","Visualization;Semantics;Interviews;Task analysis;Electronic mail;Computational modeling;Natural language processing","","1","","78","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Average Estimates in Line Graphs Are Biased Toward Areas of Higher Variability","D. Moritz; L. M. Padilla; F. Nguyen; S. L. Franconeri","Carnegie Mellon University, USA; Northeastern University, USA; Northwestern University, USA; UBC, Canada","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","306","315","We investigate variability overweighting, a previously undocumented bias in line graphs, where estimates of average value are biased toward areas of higher variability in that line. We found this effect across two preregistered experiments with 140 and 420 participants. These experiments also show that the bias is reduced when using a dot encoding of the same series. We can model the bias with the average of the data series and the average of the points drawn along the line. This bias might arise because higher variability leads to stronger weighting in the average calculation, either due to the longer line segments (even though those segments contain the same number of data values) or line segments with higher variability being otherwise more visually salient. Understanding and predicting this bias is important for visualization design guidelines, recommendation systems, and tool builders, as the bias can adversely affect estimates of averages and trends.","1941-0506","","10.1109/TVCG.2023.3326589","NSF(grant numbers:2238175,1901485); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292536","bias;lines graph;ensemble perception;average","Data visualization;Time series analysis;Encoding;Visualization;Task analysis;Ink;Market research","","","","34","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Eleven Years of Gender Data Visualization: A Step Towards More Inclusive Gender Representation","F. Cabric; M. V. Bjarnadóttir; M. Ling; G. L. Rafnsdóttir; P. Isenberg","Université Paris-Saclay, CNRS, Inria, LISN, France; Robert H. Smith School of Business, University of Maryland, College Park, USA; Ohio State University, USA; Faculty of Social and Human Science, University of Iceland, Reykjavik, Iceland; Université Paris-Saclay, CNRS, Inria, LISN, France","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","316","326","We present an analysis of the representation of gender as a data dimension in data visualizations and propose a set of considerations around visual variables and annotations for gender-related data. Gender is a common demographic dimension of data collected from study or survey participants, passengers, or customers, as well as across academic studies, especially in certain disciplines like sociology. Our work contributes to multiple ongoing discussions on the ethical implications of data visualizations. By choosing specific data, visual variables, and text labels, visualization designers may, inadvertently or not, perpetuate stereotypes and biases. Here, our goal is to start an evolving discussion on how to represent data on gender in data visualizations and raise awareness of the subtleties of choosing visual variables and words in gender visualizations. In order to ground this discussion, we collected and coded gender visualizations and their captions from five different scientific communities (Biology, Politics, Social Studies, Visualisation, and Human-Computer Interaction), in addition to images from Tableau Public and the Information Is Beautiful awards showcase. Overall we found that representation types are community-specific, color hue is the dominant visual channel for gender data, and nonconforming gender is under-represented. We end our paper with a discussion of considerations for gender visualization derived from our coding and the literature and recommendations for large data collection bodies. A free copy of this paper and all supplemental materials are available at https://osf.io/v9ams/.","1941-0506","","10.1109/TVCG.2023.3327369","Inria; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304299","Visualization;gender;visual gender representation;ethics","Data visualization;Visualization;Surveys;Image color analysis;Behavioral sciences;Ethics;Biology","Humans;Data Visualization;Computer Graphics;Surveys and Questionnaires","","","84","IEEE","1 Nov 2023","","","IEEE","IEEE Journals"
"My Model is Unfair, Do People Even Care? Visual Design Affects Trust and Perceived Bias in Machine Learning","A. Gaba; Z. Kaufman; J. Cheung; M. Shvakel; K. W. Hall; Y. Brun; C. X. Bearfield","University of Massachusetts, Amherst, USA; University of Massachusetts, Amherst, USA; University of Massachusetts, Amherst, USA; University of Massachusetts, Amherst, USA; Global Compliance, TD Bank, USA; University of Massachusetts, Amherst, USA; University of Massachusetts, Amherst, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","327","337","Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer “Can visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?” Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.","1941-0506","","10.1109/TVCG.2023.3327192","National Science Foundation(grant numbers:IIS-2237585,CCF-2210243,CCF-1763423); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296507","machine learning;fairness;bias;trust;visual design;gender;human-subjects studies","Visualization;Computational modeling;Analytical models;Data models;Bars;Investment;Games","Male;Humans;Female;Trust;Computer Graphics;Machine Learning;Bias;Surveys and Questionnaires","","","93","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"The Rational Agent Benchmark for Data Visualization","Y. Wu; Z. Guo; M. Mamakos; J. Hartline; J. Hullman","Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA; Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","338","347","Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.","1941-0506","","10.1109/TVCG.2023.3326513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290994","Evaluation;decision-making;rational agent;scoring rule","Data visualization;Task analysis;Behavioral sciences;Visualization;Benchmark testing;Bayes methods;Uncertainty","","","","33","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Vistrust: a Multidimensional Framework and Empirical Study of Trust in Data Visualizations","H. Elhamdadi; A. Stefkovics; J. Beyer; E. Moerth; H. Pfister; C. X. Bearfield; C. Nobre","UMass Amherst, USA; HUN-REN Centre for Social Sciences, USA; Harvard University, USA; Harvard Medical School, USA; Harvard University, USA; UMass Amherst, USA; University of Toronto, Canada","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","348","358","Trust is an essential aspect of data visualization, as it plays a crucial role in the interpretation and decision-making processes of users. While research in social sciences outlines the multi-dimensional factors that can play a role in trust formation, most data visualization trust researchers employ a single-item scale to measure trust. We address this gap by proposing a comprehensive, multidimensional conceptualization and operationalization of trust in visualization. We do this by applying general theories of trust from social sciences, as well as synthesizing and extending earlier work and factors identified by studies in the visualization field. We apply a two-dimensional approach to trust in visualization, to distinguish between cognitive and affective elements, as well as between visualization and data-specific trust antecedents. We use our framework to design and run a large crowd-sourced study to quantify the role of visual complexity in establishing trust in science visualizations. Our study provides empirical evidence for several aspects of our proposed theoretical framework, most notably the impact of cognition, affective responses, and individual differences when establishing trust in visualizations.","1941-0506","","10.1109/TVCG.2023.3326579","NSF(grant numbers:IIS-1901030,IIS-2237585); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308716","Trust;visualization;science;framework","Data visualization;Games;Particle measurements;Atmospheric measurements;Visualization;Investment;Computer science","","","","62","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"From Information to Choice: A Critical Inquiry Into Visualization Tools for Decision Making","E. Oral; R. Chawla; M. Wijkstra; N. Mahyar; E. Dimara","Utrecht University, Netherlands; University of Massachusetts Amherst, United States; Utrecht University, Netherlands; University of Massachusetts Amherst, United States; Utrecht University, Netherlands","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","359","369","In the face of complex decisions, people often engage in a three-stage process that spans from (1) exploring and analyzing pertinent information (intelligence); (2) generating and exploring alternative options (design); and ultimately culminating in (3) selecting the optimal decision by evaluating discerning criteria (choice). We can fairly assume that all good visualizations aid in the “intelligence” stage by enabling data exploration and analysis. Yet, to what degree and how do visualization systems currently support the other decision making stages, namely “design” and “choice”? To further explore this question, we conducted a comprehensive review of decision-focused visualization tools by examining publications in major visualization journals and conferences, including VIS, EuroVis, and CHI, spanning all available years. We employed a deductive coding method and in-depth analysis to assess whether and how visualization tools support design and choice. Specifically, we examined each visualization tool by (i) its degree of visibility for displaying decision alternatives, criteria, and preferences, and (ii) its degree of flexibility for offering means to manipulate the decision alternatives, criteria, and preferences with interactions such as adding, modifying, changing mapping, and filtering. Our review highlights the opportunities and challenges that decision-focused visualization tools face in realizing their full potential to support all stages of the decision making process. It reveals a surprising scarcity of tools that support all stages, and while most tools excel in offering visibility for decision criteria and alternatives, the degree of flexibility to manipulate these elements is often limited, and the lack of tools that accommodate decision preferences and their elicitation is notable. Based on our findings, to better support the choice stage, future research could explore enhancing flexibility levels and variety, exploring novel visualization paradigms, increasing algorithmic support, and ensuring that this automation is user-controlled via the enhanced flexibility I evels. Our curated list of the 88 surveyed visualization tools is available in the OSF link (https://osf.io/nrasz/?view_only=b92a90a34ae241449b5f2cd33383bfcb).","1941-0506","","10.1109/TVCG.2023.3326593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292614","Decision making;visualization;state of the art;review;survey;design;interaction;multi-criteria decision making;MCDM","Decision making;Data visualization;MCDM;Urban areas;Surveys;Task analysis;Filtering","Humans;Computer Graphics;Decision Making","","","106","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Heuristics for Supporting Cooperative Dashboard Design","V. Setlur; M. Correll; A. Satyanarayan; M. Tory","Tableau Research, USA; Tableau Research, USA; MIT CSAIL, USA; Northeastern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","370","380","Dashboards are no longer mere static displays of metrics; through functionality such as interaction and storytelling, they have evolved to support analytic and communicative goals like monitoring and reporting. Existing dashboard design guidelines, however, are often unable to account for this expanded scope as they largely focus on best practices for visual design. In contrast, we frame dashboard design as facilitating an analytical conversation: a cooperative, interactive experience where a user may interact with, reason about, or freely query the underlying data. By drawing on established principles of conversational flow and communication, we define the concept of a cooperative dashboard as one that enables a fruitful and productive analytical conversation, and derive a set of 39 dashboard design heuristics to support effective analytical conversations. To assess the utility of this framing, we asked 52 computer science and engineering graduate students to apply our heuristics to critique and design dashboards as part of an ungraded, opt-in homework assignment. Feedback from participants demonstrates that our heuristics surface new reasons dashboards may fail, and encourage a more fluid, supportive, and responsive style of dashboard design. Our approach suggests several compelling directions for future work, including dashboard authoring tools that better anticipate conversational turn-taking, repair, and refinement and extending cooperative principles to other analytical workflows.","1941-0506","","10.1109/TVCG.2023.3327158","NSF(grant numbers:1900991); The Roux Family Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297573","Gricean maxims;interactive visualization;conversation initiation;grounding;turn-taking;repair and refinement","Oral communication;Data visualization;Guidelines;Visualization;Maintenance engineering;Surveys;Grounding","","","","100","CCBYNCND","26 Oct 2023","","","IEEE","IEEE Journals"
"Transitioning to a Commercial Dashboarding System: Socio-Technical Observations and Opportunities","C. Walchshofer; V. Dhanoa; M. Streit; M. Meyer","Johannes Kepler University Linz, Austria; Pro2 Future GmbH, Austria; Johannes Kepler University Linz, Austria; Linköping University, Sweden","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","381","391","Many long-established, traditional manufacturing businesses are becoming more digital and data-driven to improve their production. These companies are embracing visual analytics in these transitions through their adoption of commercial dashboarding systems. Although a number of studies have looked at the technical challenges of adopting these systems, very few have focused on the socio-technical issues that arise. In this paper, we report on the results of an interview study with 17 participants working in a range of roles at a long-established, traditional manufacturing company as they adopted Microsoft Power BI. The results highlight a number of socio-technical challenges the employees faced, including difficulties in training, using and creating dashboards, and transitioning to a modern digital company. Based on these results, we propose a number of opportunities for both companies and visualization researchers to improve these difficult transitions, as well as opportunities for rethinking how we design dashboarding systems for real-world use.","1941-0506","","10.1109/TVCG.2023.3326525","Wallenberg AI, Autonomous Systems and Software Program (WASP); Knut and Alice Wallenberg Foundation; Austrian Science Fund(grant numbers:FWF DFH 23-N); Austrian Research Promotion Agency(grant numbers:FFG 881844); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296834","Interview study;socio-technical challenges;visual analytics","Companies;Data visualization;Data analysis;Interviews;Manufacturing;Surveys;Production","","1","","62","CCBY","25 Oct 2023","","","IEEE","IEEE Journals"
"Visual Analytics for Understanding Draco's Knowledge Base","J. Schmidt; B. Pointner; S. Miksch","VRVis Zentrum für Virtual Reality und visualisierung Forschungs-GmbH, Austria; VRVis Zentrum für Virtual Reality und visualisierung Forschungs-GmbH, Austria; Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","392","402","Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.","1941-0506","","10.1109/TVCG.2023.3326912","BMK; BMAW; Styria; SFG; Vienna Business Agency; FFG(grant numbers:879730); Austrian Science Foundation (FWF)(grant numbers:P31419-N31,P35767); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294308","Visual Analytics;Hypergraph visualization;Rule-based recommendation systems","Data visualization;Aggregates;Task analysis;Recommender systems;Costs;Bars;Visual analytics","","","","53","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"DIVI: Dynamically Interactive Visualization","L. S. Snyder; J. Heer","University of Washington, USA; University of Washington, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","403","413","Dynamically Interactive Visualization (DIVI) is a novel approach for orchestrating interactions within and across static visualizations. DIVI deconstructs Scalable Vector Graphics charts at runtime to infer content and coordinate user input, decoupling interaction from specification logic. This decoupling allows interactions to extend and compose freely across different tools, chart types, and analysis goals. DIVI exploits positional relations of marks to detect chart components such as axes and legends, reconstruct scales and view encodings, and infer data fields. DIVI then enumerates candidate transformations across inferred data to perform linking between views. To support dynamic interaction without prior specification, we introduce a taxonomy that formalizes the space of standard interactions by chart element, interaction type, and input event. We demonstrate DIVI's usefulness for rapid data exploration and analysis through a usability study with 13 participants and a diverse gallery of dynamically interactive visualizations, including single chart, multi-view, and cross-tool configurations.","1941-0506","","10.1109/TVCG.2023.3327172","Moore Foundation software; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299539","Interaction;Visualization Tools;Charts;SVG;Exploratory Data Analysis","Data visualization;Taxonomy;Visualization;Semantics;Brushes;Navigation;Mice","","","","40","IEEE","27 Oct 2023","","","IEEE","IEEE Journals"
"ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics","M. Kay","Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","414","424","The grammar of graphics is ubiquitous, providing the foundation for a variety of popular visualization tools and toolkits. Yet support for uncertainty visualization in the grammar graphics—beyond simple variations of error bars, uncertainty bands, and density plots—remains rudimentary. Research in uncertainty visualization has developed a rich variety of improved uncertainty visualizations, most of which are difficult to create in existing grammar of graphics implementations. ggdist, an extension to the popular ggplot2 grammar of graphics toolkit, is an attempt to rectify this situation. ggdist unifies a variety of uncertainty visualization types through the lens of distributional visualization, allowing functions of distributions to be mapped to directly to visual channels (aesthetics), making it straightforward to express a variety of (sometimes weird!) uncertainty visualization types. This distributional lens also offers a way to unify Bayesian and frequentist uncertainty visualization by formalizing the latter with the help of confidence distributions. In this paper, I offer a description of this uncertainty visualization paradigm and lessons learned from its development and adoption: ggdist has existed in some form for about six years (originally as part of the tidybayes R package for post-processing Bayesian models), and it has evolved substantially over that time, with several rewrites and API re-organizations as it changed in response to user feedback and expanded to cover increasing varieties of uncertainty visualization types. Ultimately, given the huge expressive power of the grammar of graphics and the popularity of tools built on it, I hope a catalog of my experience with ggdist will provide a catalyst for further improvements to formalizations and implementations of uncertainty visualization in grammar of graphics ecosystems. A free copy of this paper is available at https://osf.io/2gsz6. All supplemental materials are available at https://github.com/mjskay/ggdist-paper and are archived on Zenodo at doi:10.5281/zenodo.7770984.","1941-0506","","10.1109/TVCG.2023.3327195","NSF(grant numbers:2126598); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297592","Uncertainty visualization;probability distributions;confidence distributions;grammar of graphics","Uncertainty;Grammar;Visualization;Standards;Bayes methods;Geometry;Data visualization","","5","","55","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Metrics-Based Evaluation and Comparison of Visualization Notations","N. Kruchten; A. M. McNutt; M. J. McGuffin","École de technologie supérieure, Canada; University of Chicago, USA; École de technologie supérieure, Canada","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","425","435","A visualization notation is a recurring pattern of symbols used to author specifications of visualizations, from data transformation to visual mapping. Programmatic notations use symbols defined by grammars or domain-specific languages (e.g. ggplot2, dplyr, Vega-Lite) or libraries (e.g. Matplotlib, Pandas). Designers and prospective users of grammars and libraries often evaluate visualization notations by inspecting galleries of examples. While such collections demonstrate usage and expressiveness, their construction and evaluation are usually ad hoc, making comparisons of different notations difficult. More rarely, experts analyze notations via usability heuristics, such as the Cognitive Dimensions of Notations framework. These analyses, akin to structured close readings of text, can reveal design deficiencies, but place a burden on the expert to simultaneously consider many facets of often complex systems. To alleviate these issues, we introduce a metrics-based approach to usability evaluation and comparison of notations in which metrics are computed for a gallery of examples across a suite of notations. While applicable to any visualization domain, we explore the utility of our approach via a case study considering statistical graphics that explores 40 visualizations across 9 widely used notations. We facilitate the computation of appropriate metrics and analysis via a new tool called NotaScope. We gathered feedback via interviews with authors or maintainers of prominent charting libraries ($n=6$). We find that this approach is a promising way to formalize, externalize, and extend evaluations and comparisons of visualization notations.","1941-0506","","10.1109/TVCG.2023.3326907","NSERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294197","Notation;Usability;Evaluation;Language design;API design;Domain-specific languages","Measurement;Usability;Data visualization;Visualization;Libraries;Task analysis;Grammar","","","","95","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Mosaic: An Architecture for Scalable & Interoperable Data Views","J. Heer; D. Moritz","University of Washington, USA; Carnegie Mellon University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","436","446","Mosaic is an architecture for greater scalability, extensibility, and interoperability of interactive data views. Mosaic decouples data processing from specification logic: clients publish their data needs as declarative queries that are then managed and automatically optimized by a coordinator that proxies access to a scalable data store. Mosaic generalizes Vegalite's selection abstraction to enable rich integration and linking across visualizations and components such as menus, text search, and tables. We demonstrate Mosaic's expressiveness, extensibility, and interoperability through examples that compose diverse visualization, interaction, and optimization techniques—many constructed using vgplot, a grammar of interactive graphics in which graphical marks act as Mosaic clients. To evaluate scalability, we present benchmark studies with order-of-magnitude performance improvements over existing web-based visualization systems—enabling flexible, real-time visual exploration of billion+ record datasets. We conclude by discussing Mosaic's potential as an open platform that bridges visualization languages, scalable visualization, and interactive data systems more broadly.","1941-0506","","10.1109/TVCG.2023.3327189","Moore Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297587","Visualization;Interaction;Scalability;Grammar of Graphics;Software Architecture;Databases","Data visualization;Computer architecture;Soft sensors;Scalability;Databases;Optimization;Grammar","","","","49","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Mystique: Deconstructing SVG Charts for Layout Reuse","C. Chen; B. Lee; Y. Wang; Y. Chang; Z. Liu","University of Maryland, College Park, Maryland, United States; Microsoft Research, Redmond, Washington, United States; Shandong University, Qingdao, China; University of Maryland, College Park, Maryland, United States; University of Maryland, College Park, Maryland, United States","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","447","457","To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.","1941-0506","","10.1109/TVCG.2023.3327354","NSF(grant numbers:IIS-2239130); NSFC(grant numbers:62132017,62141217); Shandong Provincial Natural Science Foundation(grant numbers:ZQ2022JQ32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297572","Chart layout;Reuse;Reverse-engineering;Deconstruction","Layout;Data visualization;Bars;Visualization;Data mining;Encoding;Semantics","","","","48","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"VIRD: Immersive Match Video Analysis for High-Performance Badminton Coaching","T. Lin; A. Aouididi; Z. Chen; J. Beyer; H. Pfister; J. -H. Wang","Harvard John A. Paulson School of Engineering and Applied Sciences, USA; Harvard John A. Paulson School of Engineering and Applied Sciences, USA; Harvard John A. Paulson School of Engineering and Applied Sciences, USA; Harvard John A. Paulson School of Engineering and Applied Sciences, USA; Harvard John A. Paulson School of Engineering and Applied Sciences, USA; Adobe Research, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","458","468","Badminton is a fast-paced sport that requires a strategic combination of spatial, temporal, and technical tactics. To gain a competitive edge at high-level competitions, badminton professionals frequently analyze match videos to gain insights and develop game strategies. However, the current process for analyzing matches is time-consuming and relies heavily on manual note-taking, due to the lack of automatic data collection and appropriate visualization tools. As a result, there is a gap in effectively analyzing matches and communicating insights among badminton coaches and players. This work proposes an end-to-end immersive match analysis pipeline designed in close collaboration with badminton professionals, including Olympic and national coaches and players. We present VIRD, a VR Bird (i.e., shuttle) immersive analysis tool, that supports interactive badminton game analysis in an immersive environment based on 3D reconstructed game views of the match video. We propose a top-down analytic workflow that allows users to seamlessly move from a high-level match overview to a detailed game view of individual rallies and shots, using situated 3D visualizations and video. We collect 3D spatial and dynamic shot data and player poses with computer vision models and visualize them in VR. Through immersive visualizations, coaches can interactively analyze situated spatial data (player positions, poses, and shot trajectories) with flexible viewpoints while navigating between shots and rallies effectively with embodied interaction. We evaluated the usefulness of VIRD with Olympic and national-level coaches and players in real matches. Results show that immersive analytics supports effective badminton match analysis with reduced context-switching costs and enhances spatial understanding with a high sense of presence.","1941-0506","","10.1109/TVCG.2023.3327161","Adobe Research; NSF(grant numbers:III-2107328,IIS-1901030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296031","Sports Analytics;Immersive Analytics;Data Visualization","Sports;Games;Data visualization;Three-dimensional displays;Trajectory;Training;Spatial databases","Mentoring;Computer Graphics;Racquet Sports","","","64","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network Visualisations in Virtual Reality","S. P. Feyer; B. Pinaud; S. Kobourov; N. Brich; M. Krone; A. Kerren; M. Behrisch; F. Schreiber; K. Klein","Life Science Informatics, University of Konstanz, Germany; Univ. Bordeaux, CNRS, Bordeaux INP, LaBRI, UMR 5800, France; University of Arizona, USA; University of Tübingen, Germany; University of Tübingen, Germany; Linköping University, Sweden; Utrecht University, NL; University of Konstanz, Germany; Life Science Informatics, University of Konstanz, Germany","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","469","479","Relational information between different types of entities is often modelled by a multilayer network (MLN) – a network with subnetworks represented by layers. The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.","1941-0506","","10.1109/TVCG.2023.3327402","DFG(grant numbers:422037984,251654672); National Science Foundation(grant numbers:NSF-CCF-2212130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298000","Network;Guidelines;VisDesign;HumanQuant;CompSystems","Task analysis;Visualization;Three-dimensional displays;Data visualization;Taxonomy;Nonhomogeneous media;Surveys","","","","67","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"MeTACAST: Target- and Context-Aware Spatial Selection in VR","L. Zhao; T. Isenberg; F. Xie; H. -N. Liang; L. Yu","Xi'an Jiaotong-Liverpool University, China; Université Paris-Saclay, CNRS, Inria, LISN, France; Xi'an Jiaotong-Liverpool University, China; Xi'an Jiaotong-Liverpool University, China; Xi'an Jiaotong-Liverpool University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","480","494","We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters—with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration—with simple and approximate 3D pointing, brushing, or drawing input—using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment.","1941-0506","","10.1109/TVCG.2023.3326517","NSFC(grant numbers:62272396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292508","Spatial selection;immersive analytics;virtual reality (VR);target-aware and context-aware interaction for visualization","Three-dimensional displays;Shape;Density measurement;Volume measurement;Data visualization;Particle measurements;Spatial databases","","","","65","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Unraveling the Design Space of Immersive Analytics: A Systematic Review","D. Saffo; S. Di Bartolomeo; T. Crnovrsanin; L. South; J. Raynor; C. Yildirim; C. Dunne","Northeastern University, USA; Northeastern University, USA; Northeastern University, USA; Northeastern University, USA; Northeastern University, USA; Northeastern University, USA; Northeastern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","495","506","Immersive analytics has emerged as a promising research area, leveraging advances in immersive display technologies and techniques, such as virtual and augmented reality, to facilitate data exploration and decision-making. This paper presents a systematic literature review of 73 studies published between 2013-2022 on immersive analytics systems and visualizations, aiming to identify and categorize the primary dimensions influencing their design. We identified five key dimensions:  Academic Theory and Contribution,  Immersive Technology,  Data,  Spatial Presentation, and  Visual Presentation. Academic Theory and Contribution assess the motivations behind the works and their theoretical frameworks. Immersive Technology examines the display and input modalities, while Data dimension focuses on dataset types and generation. Spatial Presentation discusses the environment, space, embodiment, and collaboration aspects in IA, and Visual Presentation explores the visual elements, facet and position, and manipulation of views. By examining each dimension individually and cross-referencing them, this review uncovers trends and relationships that help inform the design of immersive systems visualizations. This analysis provides valuable insights for researchers and practitioners, offering guidance in designing future immersive analytics systems and shaping the trajectory of this rapidly evolving field. A free copy of this paper and all supplemental materials are available at osf.io/5ewaj.","1941-0506","","10.1109/TVCG.2023.3327368","Northeastern University; Global Technology Applied Research Center of J.P. Morgan Chase & Co.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296039","Immersive Analytics;Systematic Review;Survey;Augmented Reality;Virtual Reality;Design Space","Data visualization;Three-dimensional displays;Surveys;Systematics;Taxonomy;Market research;Collaboration","","1","","94","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Wizualization: A “Hard Magic” Visualization System for Immersive and Ubiquitous Analytics","A. Batch; P. W. S. Butcher; P. D. Ritsos; N. Elmqvist","U.S. Bureau of Economic Analysis, Washington, D.C., United States; Bangor University, Bangor, United Kingdom; Bangor University, Bangor, United Kingdom; Aarhus University, Aarhus, Denmark","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","507","517","What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (Spellbook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.","1941-0506","","10.1109/TVCG.2023.3326580","U.S. National Science Foundation(grant numbers:IIS-1908605); DSP Centre; ERDF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294315","Immersive analytics;situated analytics;ubiquitous analytics;gestural interaction;voice interaction","Data visualization;Three-dimensional displays;Grammar;X reality;Surveys;Rendering (computer graphics);Filtering","","","","82","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers of Science and Technology","Y. Wang; Y. Qian; X. Qi; N. Cao; D. Wang","The Center for Science of Science and Innovation, Northwestern University, USA; The Center for Science of Science and Innovation, Northwestern University, USA; Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China; The Center for Science of Science and Innovation, Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","518","528","Science has long been viewed as a key driver of economic growth and rising standards of living. Knowledge about how scientific advances support marketplace inventions is therefore essential for understanding the role of science in propelling real-world applications and technological progress. The increasing availability of large-scale datasets tracing scientific publications and patented inventions and the complex interactions among them offers us new opportunities to explore the evolving dual frontiers of science and technology at an unprecedented level of scale and detail. However, we lack suitable visual analytics approaches to analyze such complex interactions effectively. Here we introduce InnovationInsights, an interactive visual analysis system for researchers, research institutions, and policymakers to explore the complex linkages between science and technology, and to identify critical innovations, inventors, and potential partners. The system first identifies important associations between scientific papers and patented inventions through a set of statistical measures introduced by our experts from the field of the Science of Science. A series of visualization views are then used to present these associations in the data context. In particular, we introduce the Interplay Graph to visualize patterns and insights derived from the data, helping users effectively navigate citation relationships between papers and patents. This visualization thereby helps them identify the origins of technical inventions and the impact of scientific research. We evaluate the system through two case studies with experts followed by expert interviews. We further engage a premier research institution to test-run the system, helping its institution leaders to extract new insights for innovation. Through both the case studies and the engagement project, we find that our system not only meets our original goals of design, allowing users to better identify the sources of technical inventions and to understand the broad impact of scientific research; it also goes beyond these purposes to enable an array of new applications for researchers and research institutions, ranging from identifying untapped innovation potential within an institution to forging new collaboration opportunities between science and industry.","1941-0506","","10.1109/TVCG.2023.3327387","Air Force Office of Scientific Research(grant numbers:FA9550-17-1-0089,FA9550-19-1-0354); Alfred P. Sloan Foundation(grant numbers:G-2019-12485); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296058","Science of Science;Innovation;Academic Profiles;Patent Data;Publication Data;Visual Analytics","Data visualization;Patents;Technological innovation;Visual analytics;Collaboration;Stakeholders;Industries","","","","78","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"LiberRoad: Probing into the Journey of Chinese Classics Through Visual Analytics","Y. Guo; Y. Luo; K. Lu; L. Li; H. Yang; X. Yuan","Key Laboratory of Machine Perception (Ministry of Education), School of AI, Peking University, China; Key Laboratory of Machine Perception (Ministry of Education), School of AI, Peking University, China; Key Laboratory of Machine Perception (Ministry of Education), School of AI, Peking University, China; Department of Chinese Language & Literature, Center for Ancient Chinese Classics & Archives, Peking University, China; Department of Chinese Language & Literature, Center for Ancient Chinese Classics & Archives, Peking University, China; Key Laboratory of Machine Perception (Ministry of Education), School of AI, Peking University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","529","539","Books act as a crucial carrier of cultural dissemination in ancient times. This work involves joint efforts between visualization and humanities researchers, aiming at building a holistic view of the cultural exchange and integration between China and Japan brought about by the overseas circulation of Chinese classics. Book circulation data consist of uncertain spatiotemporal trajectories, with multiple dimensions, and movement across hierarchical spaces forms a compound network. LiberRoad visualizes the circulation of books collected in the Imperial Household Agency of Japan, and can be generalized to other book movement data. The LiberRoad system enables a smooth transition between three views (Location Graph, map, and timeline) according to the desired perspectives (spatial or temporal), as well as flexible filtering and selection. The Location Graph is a novel uncertainty-aware visualization method that employs improved circle packing to represent spatial hierarchy. The map view intuitively shows the overall circulation by clustering and allows zooming into single book trajectory with lenses magnifying local movements. The timeline view ranks dynamically in response to user interaction to facilitate the discovery of temporal events. The evaluation and feedback from the expert users demonstrate that LiberRoad is helpful in revealing movement patterns and comparing circulation characteristics of different times and spaces.","1941-0506","","10.1109/TVCG.2023.3326944","NSFC(grant numbers:62272012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294205","Visual analytics;digital humanities;spatial uncertainty;trajectory visualization;book movement;historical data","Trajectory;Cultural differences;Data visualization;Task analysis;History;Uncertainty;Visual analytics","","","","68","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Visualizing Historical Book Trade Data: An Iterative Design Study with Close Collaboration with Domain Experts","Y. Xing; C. Dondi; R. Borgo; A. Abdul-Rahman","King's College London, United Kingdom; University of Oxford, United Kingdom; King's College London, United Kingdom; King's College London, United Kingdom","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","540","550","The circulation of historical books has always been an area of interest for historians. However, the data used to represent the journey of a book across different places and times can be difficult for domain experts to digest due to buried geographical and chronological features within text-based presentations. This situation provides an opportunity for collaboration between visualization researchers and historians. This paper describes a design study where a variant of the Nine-Stage Framework [46] was employed to develop a Visual Analytics (VA) tool called DanteExploreVis. This tool was designed to aid domain experts in exploring, explaining, and presenting book trade data from multiple perspectives. We discuss the design choices made and how each panel in the interface meets the domain requirements. We also present the results of a qualitative evaluation conducted with domain experts. The main contributions of this paper include: 1) the development of a VA tool to support domain experts in exploring, explaining, and presenting book trade data; 2) a comprehensive documentation of the iterative design, development, and evaluation process following the variant Nine-Stage Framework; 3) a summary of the insights gained and lessons learned from this design study in the context of the humanities field; and 4) reflections on how our approach could be applied in a more generalizable way.","1941-0506","","10.1109/TVCG.2023.3326923","King's-China Scholarship Council PhD Scholarship programme (K-CSC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292899","Design study;application motivated visualization;geospatial data","Data visualization;Iterative methods;Collaboration;Usability;Task analysis;Trajectory;Printing","","","","59","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"OldVisOnline: Curating a Dataset of Historical Visualizations","Y. Zhang; R. Jiang; L. Xie; Y. Zhao; C. Liu; T. Ding; S. Chen; X. Yuan","Department of Computer Science, University of Oxford, United Kingdom; Key Laboratory of Machine Perception (Ministry of Education), School of Intelligence Science and Technology, Peking University, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, China; School of Data Science, Fudan University, China; Key Laboratory of Machine Perception (Ministry of Education), School of Intelligence Science and Technology, Peking University, China; Fundamental Software Innovation Lab, Huawei Technologies, China; School of Data Science, Fudan University, China; Key Laboratory of Machine Perception (Ministry of Education), School of Intelligence Science and Technology, Peking University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","551","561","With the increasing adoption of digitization, more and more historical visualizations created hundreds of years ago are accessible in digital libraries online. It provides a unique opportunity for visualization and history research. Meanwhile, there is no large-scale digital collection dedicated to historical visualizations. The visualizations are scattered in various collections, which hinders retrieval. In this study, we curate the first large-scale dataset dedicated to historical visualizations. Our dataset comprises 13K historical visualization images with corresponding processed metadata from seven digital libraries. In curating the dataset, we propose a workflow to scrape and process heterogeneous metadata. We develop a semi-automatic labeling approach to distinguish visualizations from other artifacts. Our dataset can be accessed with OldVisOnline, a system we have built to browse and label historical visualizations. We discuss our vision of usage scenarios and research opportunities with our dataset, such as textual criticism for historical visualizations. Drawing upon our experience, we summarize recommendations for future efforts to improve our dataset.","1941-0506","","10.1109/TVCG.2023.3326908","NSFC(grant numbers:62272012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295430","Historical visualization;dataset;digital humanities;data labeling","Data visualization;Libraries;Metadata;History;Soft sensors;Search engines;Labeling","","","","91","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams","Q. Chen; N. Chen; W. Shuai; G. Wu; Z. Xu; H. Tong; N. Cao","Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China; New York University, USA; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; Intelligent Big Data Visualization Lab, Tongji University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","562","572","Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.","1941-0506","","10.1109/TVCG.2023.3326925","NSFC(grant numbers:62002267,62072338,62061136003); NSF(grant numbers:23ZR1464700); Shanghai Education Development Foundation(grant numbers:21CGA75); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295417","Graph Data;Application Motivated Visualization;Automatic Visualization;Narrative Visualization;Authoring Tools","Data visualization;Annotations;Data mining;Visualization;Organizations;Layout;Interviews","","","","78","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Quantivine: A Visualization Approach for Large-Scale Quantum Circuit Representation and Analysis","Z. Wen; Y. Liu; S. Tan; J. Chen; M. Zhu; D. Han; J. Yin; M. Xu; W. Chen","State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; Advanced Computing and System Laboratory, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; Zhejiang University, China; Hithink RoyalFlush Information Network Co. Ltd. Zhejiang, China; Advanced Computing and System Laboratory, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","573","583","Quantum computing is a rapidly evolving field that enables exponential speed-up over classical algorithms. At the heart of this revolutionary technology are quantum circuits, which serve as vital tools for implementing, analyzing, and optimizing quantum algorithms. Recent advancements in quantum computing and the increasing capability of quantum devices have led to the development of more complex quantum circuits. However, traditional quantum circuit diagrams suffer from scalability and readability issues, which limit the efficiency of analysis and optimization processes. In this research, we propose a novel visualization approach for large-scale quantum circuits by adopting semantic analysis to facilitate the comprehension of quantum circuits. We first exploit meta-data and semantic information extracted from the underlying code of quantum circuits to create component segmentations and pattern abstractions, allowing for easier wrangling of massive circuit diagrams. We then develop Quantivine, an interactive system for exploring and understanding quantum circuits. A series of novel circuit visualizations is designed to uncover contextual details such as qubit provenance, parallelism, and entanglement. The effectiveness of Quantivine is demonstrated through two usage scenarios of quantum circuits with up to 100 qubits and a formal user evaluation with quantum experts. A free copy of this paper and all supplemental materials are available at https://osf.io/2m9yh/?view_only=0aa1618c97244f5093cd7ce15f1431f9.","1941-0506","","10.1109/TVCG.2023.3327148","National Natural Science Foundation of China(grant numbers:62132017); Zhejiang Pioneer (Jianbing) Project(grant numbers:2023C01036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296051","Quantum circuit;semantic analysis;visual abstraction;context visualization","Quantum circuit;Visualization;Qubit;Semantics;Logic gates;Codes;Quantum algorithm","","","","83","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Knowledge Graphs in Practice: Characterizing their Users, Challenges, and Visualization Opportunities","H. Li; G. Appleby; C. D. Brumar; R. Chang; A. Suh","MIT Lincoln Laboratory, USA; Tufts University, USA; Tufts University, USA; Tufts University, USA; MIT Lincoln Laboratory, USA","IEEE Transactions on Visualization and Computer Graphics","28 Dec 2023","2024","30","1","584","594","This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners – KG Builders, Analysts, and Consumers – each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, technologies, and collaborative workflows. From the analysis of our interviews, we distill several visualization research directions to improve KG usability, including knowledge cards that balance digestibility and discoverability, timeline views to track temporal changes, interfaces that support organic discovery, and semantic explanations for AI and machine learning predictions.","1941-0506","","10.1109/TVCG.2023.3326904","National Science Foundation(grant numbers:IIS1452977,OAC-1940175,OAC-1939945,OAC-2118201,NRT-2021874); Department of the Air Force(grant numbers:FA8702-15-D-0001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10360419","Knowledge graphs;visualization techniques and methodologies;human factors;visual communication","Knowledge graphs;Data visualization;Interviews;Task analysis;Data models;Standards organizations;Semantics","","","","81","IEEE","14 Dec 2023","","","IEEE","IEEE Journals"
"Scalable Hypergraph Visualization","P. Oliver; E. Zhang; Y. Zhang","School of Electrical Engineering and Computer Science, Oregon State University, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","595","605","Hypergraph visualization has many applications in network data analysis. Recently, a polygon-based representation for hypergraphs has been proposed with demonstrated benefits. However, the polygon-based layout often suffers from excessive self-intersections when the input dataset is relatively large. In this paper, we propose a framework in which the hypergraph is iteratively simplified through a set of atomic operations. Then, the layout of the simplest hypergraph is optimized and used as the foundation for a reverse process that brings the simplest hypergraph back to the original one, but with an improved layout. At the core of our approach is the set of atomic simplification operations and an operation priority measure to guide the simplification process. In addition, we introduce necessary definitions and conditions for hypergraph planarity within the polygon representation. We extend our approach to handle simultaneous simplification and layout optimization for both the hypergraph and its dual. We demonstrate the utility of our approach with datasets from a number of real-world applications.","1941-0506","","10.1109/TVCG.2023.3326599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290912","Hypergraph visualization;scalable visualization;polygon layout;hypergraph embedding;primal-dual visualization","Atomic measurements;Data analysis;Layout;Data visualization;Optimization","","","","57","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"SpeechMirror: A Multimodal Visual Analytics System for Personalized Reflection of Online Public Speaking Effectiveness","Z. Huang; Q. He; K. Maher; X. Deng; Y. -K. Lai; C. Ma; S. -F. Qin; Y. -J. Liu; H. Wang","Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences, China; Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences, China; Diatom Design Limited Liability Company, USA; Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences, China; School of Computer Science and Informatics, Cardiff University, United Kingdom; Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences, China; School of Design, Northumbria University, United Kingdom; Department of Computer Science and Technology, MOE-Key Laboratory of Pervasive Computing, Tsinghua University, China; Beijing Key Laboratory of Human-Computer Interaction, Institute of Software, Chinese Academy of Sciences, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","606","616","As communications are increasingly taking place virtually, the ability to present well online is becoming an indispensable skill. Online speakers are facing unique challenges in engaging with remote audiences. However, there has been a lack of evidence-based analytical systems for people to comprehensively evaluate online speeches and further discover possibilities for improvement. This paper introduces SpeechMirror, a visual analytics system facilitating reflection on a speech based on insights from a collection of online speeches. The system estimates the impact of different speech techniques on effectiveness and applies them to a speech to give users awareness of the performance of speech techniques. A similarity recommendation approach based on speech factors or script content supports guided exploration to expand knowledge of presentation evidence and accelerate the discovery of speech delivery possibilities. SpeechMirror provides intuitive visualizations and interactions for users to understand speech factors. Among them, SpeechTwin, a novel multimodal visual summary of speech, supports rapid understanding of critical speech factors and comparison of different speech samples, and SpeechPlayer augments the speech video by integrating visualization of the speaker's body language with interaction, for focused analysis. The system utilizes visualizations suited to the distinct nature of different speech factors for user comprehension. The proposed system and visualization techniques were evaluated with domain experts and amateurs, demonstrating usability for users with low visualization literacy and its efficacy in assisting users to develop insights for potential improvement.","1941-0506","","10.1109/TVCG.2023.3326932","National Key R&D Program of China(grant numbers:2022ZD0117900); Beijing Natural Science Foundation(grant numbers:4212029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292871","Visual Analytics;Multimodal Analysis;Public Speaking;Online Presentation","Speech;Public speaking;Visual analytics;Interviews;Data visualization;Speech enhancement;Reflection","Humans;Speech;Computer Graphics;Communication","","","71","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"VISGRADER: Automatic Grading of D3 Visualizations","M. Hull; V. Pednekar; H. Murray; N. Roy; E. Tung; S. Routray; C. Guerin; J. Chen; Z. J. Wang; S. Lee; M. Roozbahani; D. H. Chau","Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA; Georgia Institute of Technology, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","617","627","Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VISGRADER, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students' learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.","1941-0506","","10.1109/TVCG.2023.3327181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297575","Automatic grading;D3 visualization;large class;Selenium;Gradescope grading platform","Data visualization;Image color analysis;Codes;Task analysis;Programming;Manuals;Education","","","","49","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Adaptive Assessment of Visualization Literacy","Y. Cui; L. W. Ge; Y. Ding; F. Yang; L. Harrison; M. Kay","Northwestern University, USA; Northwestern University, USA; Worcester Polytechnic Institute, USA; Northwestern University, USA; Worcester Polytechnic Institute, USA; Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","628","637","Visualization literacy is an essential skill for accurately interpreting data to inform critical decisions. Consequently, it is vital to understand the evolution of this ability and devise targeted interventions to enhance it, requiring concise and repeatable assessments of visualization literacy for individuals. However, current assessments, such as the Visualization Literacy Assessment Test (vlat), are time-consuming due to their fixed, lengthy format. To address this limitation, we develop two streamlined computerized adaptive tests (cats) for visualization literacy, a-vlat and a-calvi, which measure the same set of skills as their original versions in half the number of questions. Specifically, we (1) employ item response theory (IRT) and non-psychometric constraints to construct adaptive versions of the assessments, (2) finalize the configurations of adaptation through simulation, (3) refine the composition of test items of a-calvi via a qualitative study, and (4) demonstrate the test-retest reliability (ICC: 0.98 and 0.98) and convergent validity (correlation: 0.81 and 0.66) of both CATS via four online studies. We discuss practical recommendations for using our CATS and opportunities for further customization to leverage the full potential of adaptive assessments. All supplemental materials are available at https://osf.io/a6258/.","1941-0506","","10.1109/TVCG.2023.3327165","National Science Foundation(grant numbers:1815587,2120750,2127309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295997","Visualization literacy;computerized adaptive testing;item response theory","Data visualization;Testing;Reliability;Visualization;Germanium;Fake news;Task analysis","","","","33","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Causality-Based Visual Analysis of Questionnaire Responses","R. Li; W. Cui; T. Song; X. Xie; R. Ding; Y. Wang; H. Zhang; H. Zhou; Y. Wu","State Key Lab of CAD&CG, Zhejiang University, China; Microsoft Research Asia, China; State Key Lab of CAD&CG, Zhejiang University, China; Department of Sports Science, Zhejiang University, China; Microsoft Research Asia, China; Microsoft Research Asia, China; Microsoft Research Asia, China; College of Computer Science and Software Engineering, Shenzhen University, China; State Key Lab of CAD&CG, Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","638","648","As the final stage of questionnaire analysis, causal reasoning is the key to turning responses into valuable insights and actionable items for decision-makers. During the questionnaire analysis, classical statistical methods (e.g., Differences-in-Differences) have been widely exploited to evaluate causality between questions. However, due to the huge search space and complex causal structure in data, causal reasoning is still extremely challenging and time-consuming, and often conducted in a trial-and-error manner. On the other hand, existing visual methods of causal reasoning face the challenge of bringing scalability and expert knowledge together and can hardly be used in the questionnaire scenario. In this work, we present a systematic solution to help analysts effectively and efficiently explore questionnaire data and derive causality. Based on the association mining algorithm, we dig question combinations with potential inner causality and help analysts interactively explore the causal sub-graph of each question combination. Furthermore, leveraging the requirements collected from the experts, we built a visualization tool and conducted a comparative study with the state-of-the-art system to show the usability and efficiency of our system.","1941-0506","","10.1109/TVCG.2023.3327376","National Key R&D Program of China(grant numbers:2022YFE0137800); NSFC(grant numbers:62072400,62202424); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302175","Causal analysis;Questionnaire;Design study","Cognition;Interviews;Scalability;Pipelines;Visual analytics;Correlation;Analytical models","Computer Graphics;Causality;Algorithms;Surveys and Questionnaires","","","48","IEEE","30 Oct 2023","","","IEEE","IEEE Journals"
"Challenges and Opportunities in Data Visualization Education: A Call to Action","B. Bach; M. Keck; F. Rajabiyazdi; T. Losev; I. Meirelles; J. Dykes; R. S. Laramee; M. AlKadi; C. Stoiber; S. Huron; C. Perin; L. Morais; W. Aigner; D. Kosminsky; M. Boucher; S. Knudsen; A. Manataki; J. Aerts; U. Hinrichs; J. C. Roberts; S. Carpendale","University of Edinburgh, United Kingdom; University of Applied Sciences Upper Austria, Austria; Carleton University, Canada; Simon Fraser University, Canada; OCAD University, Canada; City University London, United Kingdom; University of Nottingham, United Kingdom; University of Edinburgh, United Kingdom; University of Applied Sciences St. Pölten, Austria; Télécom Paris, France; University of Victoria, Canada; Universidade Federal de Pernambuco, Brazil; University of Applied Sciences St. Pölten, Austria; Universidade Federal de Rio de Janeiro, Brazil; University of Applied Sciences St. Pölten, Austria; University of Copenhagen, Denmark; University of Edinburgh, United Kingdom; Hasselt University, Belgium; University of Edinburgh, United Kingdom; Bangor University, United Kingdom; Simon Fraser University, Canada","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","649","660","This paper is a call to action for research and discussion on data visualization education. As visualization evolves and spreads through our professional and personal lives, we need to understand how to support and empower a broad and diverse community of learners in visualization. Data Visualization is a diverse and dynamic discipline that combines knowledge from different fields, is tailored to suit diverse audiences and contexts, and frequently incorporates tacit knowledge. This complex nature leads to a series of interrelated challenges for data visualization education. Driven by a lack of consolidated knowledge, overview, and orientation for visualization education, the 21 authors of this paper—educators and researchers in data visualization—identify and describe 19 challenges informed by our collective practical experience. We organize these challenges around seven themes People, Goals & Assessment, Environment, Motivation, Methods, Materials, and Change. Across these themes, we formulate 43 research questions to address these challenges. As part of our call to action, we then conclude with 5 cross-cutting opportunities and respective action items: embrace DIVERSITY+INCLUSION, build COMMUNITIES, conduct RESEARCH, act AGILE, and relish RESPONSIBILITY. We aim to inspire researchers, educators and learners to drive visualization education forward and discuss why, how, who and where we educate, as we learn to use visualization to address challenges across many scales and many domains in a rapidly changing world: viseducationchallenges.github.io.","1941-0506","","10.1109/TVCG.2023.3327378","EPSRC(grant numbers:EP/S010238/2,EP/V010662/1); NSERC(grant numbers:RGPIN-2021-04222); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10310184","Data Visualization;Education;Challenges","Data visualization;Education;Visualization;Seminars;Art;Cultural differences;Creativity","","2","","138","IEEE","7 Nov 2023","","","IEEE","IEEE Journals"
"A Comparative Visual Analytics Framework for Evaluating Evolutionary Processes in Multi-Objective Optimization","Y. Huang; Z. Zhang; A. Jiao; Y. Ma; R. Cheng","Department of Computer Science and Engineering, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China; Department of Computer Science and Engineering, Southern University of Science and Technology, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","661","671","Evolutionary multi-objective optimization (EMO) algorithms have been demonstrated to be effective in solving multi-criteria decision-making problems. In real-world applications, analysts often employ several algorithms concurrently and compare their solution sets to gain insight into the characteristics of different algorithms and explore a broader range of feasible solutions. However, EMO algorithms are typically treated as black boxes, leading to difficulties in performing detailed analysis and comparisons between the internal evolutionary processes. Inspired by the successful application of visual analytics tools in explainable AI, we argue that interactive visualization can significantly enhance the comparative analysis between multiple EMO algorithms. In this paper, we present a visual analytics framework that enables the exploration and comparison of evolutionary processes in EMO algorithms. Guided by a literature review and expert interviews, the proposed framework addresses various analytical tasks and establishes a multi-faceted visualization design to support the comparative analysis of intermediate generations in the evolution as well as solution sets. We demonstrate the effectiveness of our framework through case studies on benchmarking and real-world multi-objective optimization problems to elucidate how analysts can leverage our framework to inspect and compare diverse algorithms.","1941-0506","","10.1109/TVCG.2023.3326921","National Natural Science Foundation of China(grant numbers:62202217); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2023A1515012889); Guangdong Talent Program(grant numbers:2021QN02X794); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X386); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294213","Visual analytics;evolutionary multi-objective optimization","Optimization;Analytical models;Visual analytics;Task analysis;Predictive models;Computational modeling;Machine learning algorithms","","","","79","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research","H. Subramonyam; J. Hullman","Stanford University, USA; Northwestern University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","672","682","Visualization for machine learning (VIS4ML) research aims to help experts apply their prior knowledge to develop, understand, and improve the performance of machine learning models. In conceiving VIS4ML systems, researchers characterize the nature of human knowledge to support human-in-the-loop tasks, design interactive visualizations to make ML components interpretable and elicit knowledge, and evaluate the effectiveness of human-model interchange. We survey recent VIS4ML papers to assess the generalizability of research contributions and claims in enabling human-in-the-loop ML. Our results show potential gaps between the current scope of VIS4ML research and aspirations for its use in practice. We find that while papers motivate that VIS4ML systems are applicable beyond the specific conditions studied, conclusions are often overfitted to non-representative scenarios, are based on interactions with a small set of ML experts and well-understood datasets, fail to acknowledge crucial dependencies, and hinge on decisions that lack justification. We discuss approaches to close the gap between aspirations and research claims and suggest documentation practices to report generality constraints that better acknowledge the exploratory nature of VIS4ML research.","1941-0506","","10.1109/TVCG.2023.3326591","NSF(grant numbers:IIS-2211939); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290989","VIS4ML;Visualization;Machine learning;Human-in-the-loop;Human Knowledge;Generalizability;Survey","Pipelines;Machine learning;Task analysis;Surveys;Human in the loop;Data visualization;Visual analytics","","","","93","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Explore Your Network in Minutes: A Rapid Prototyping Toolkit for Understanding Neural Networks with Visual Analytics","S. Lai; W. Luan; J. Tao","School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, National Supercomputer Center, Guangzhou, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","683","693","Neural networks attract significant attention in almost every field due to their widespread applications in various tasks. However, developers often struggle with debugging due to the black-box nature of neural networks. Visual analytics provides an intuitive way for developers to understand the hidden states and underlying complex transformations in neural networks. Existing visual analytics tools for neural networks have been demonstrated to be effective in providing useful hints for debugging certain network architectures. However, these approaches are often architecture-specific with strong assumptions of how the network should be understood. This limits their use when the network architecture or the exploration goal changes. In this paper, we present a general model and a programming toolkit, Neural Network Visualization Builder (NNVisBuilder), for prototyping visual analytics systems to understand neural networks. NNVisBuilder covers the common data transformation and interaction model involved in existing tools for exploring neural networks. It enables developers to customize a visual analytics interface for answering their specific questions about networks. NNVisBuilder is compatible with PyTorch so that developers can integrate the visualization code into their learning code seamlessly. We demonstrate the applicability by reproducing several existing visual analytics systems for networks with NNVisBuilder. The source code and some example cases can be found at https://github.com/sysuvis/NVB.","1941-0506","","10.1109/TVCG.2023.3326575","National Key R&D Program of China(grant numbers:2021YFB0300103); National Natural Science Foundation of China(grant numbers:61902446,62172456,91937302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308640","Visualization model;toolkit;neural networks;visual diagnosis","Data visualization;Neural networks;Visual analytics;Data models;Artificial neural networks;Neurons;Tensors","","","","50","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples","S. Jamonnak; J. Guo; W. He; L. Gou; L. Ren","Bosch Research North America, USA; Bosch Research North America, USA; Bosch Research North America, USA; Bosch Research North America, USA; Bosch Research North America, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","694","704","Open-world object detection (OWOD) is an emerging computer vision problem that involves not only the identification of predefined object classes, like what general object detectors do, but also detects new unknown objects simultaneously. Recently, several end-to-end deep learning models have been proposed to address the OWOD problem. However, these approaches face several challenges: a) significant changes in both network architecture and training procedure are required; b) they are trained from scratch, which can not leverage existing pre-trained general detectors; c) costly annotations for all unknown classes are needed. To overcome these challenges, we present a visual analytic framework called OW-Adapter. It acts as an adaptor to enable pre-trained general object detectors to handle the OWOD problem. Specifically, OW-Adapter is designed to identify, summarize, and annotate unknown examples with minimal human effort. Moreover, we introduce a lightweight classifier to learn newly annotated unknown classes and plug the classifier into pre-trained general detectors to detect unknown objects. We demonstrate the effectiveness of our framework through two case studies of different domains, including common object recognition and autonomous driving. The studies show that a simple yet powerful adaptor can extend the capability of pre-trained general detectors to detect unknown objects and improve the performance on known classes simultaneously.","1941-0506","","10.1109/TVCG.2023.3326577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290904","Open world learning;object detection;continuous learning;human-assisted AI","Detectors;Automobiles;Proposals;Object detection;Object recognition;Visual analytics;Training","","","","76","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Dr. KID: Direct Remeshing and K-Set Isometric Decomposition for Scalable Physicalization of Organic Shapes","D. Khan; C. Bohak; I. Viola","Visual Computing Center, King Abdullah University of Science and Technology, Saudi Arabia; Visual Computing Center, King Abdullah University of Science and Technology, Saudi Arabia; Visual Computing Center, King Abdullah University of Science and Technology, Saudi Arabia","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","705","715","Dr. KID is an algorithm that uses isometric decomposition for the physicalization of potato-shaped organic models in a puzzle fashion. The algorithm begins with creating a simple, regular triangular surface mesh of organic shapes, followed by iterative K-means clustering and remeshing. For clustering, we need similarity between triangles (segments) which is defined as a distance function. The distance function maps each triangle's shape to a single point in the virtual 3D space. Thus, the distance between the triangles indicates their degree of dissimilarity. K-means clustering uses this distance and sorts segments into $k$ classes. After this, remeshing is applied to minimize the distance between triangles within the same cluster by making their shapes identical. Clustering and remeshing are repeated until the distance between triangles in the same cluster reaches an acceptable threshold. We adopt a curvature-aware strategy to determine the surface thickness and finalize puzzle pieces for 3D printing. Identical hinges and holes are created for assembling the puzzle components. For smoother outcomes, we use triangle subdivision along with curvature-aware clustering, generating curved triangular patches for 3D printing. Our algorithm was evaluated using various models, and the 3D-printed results were analyzed. Findings indicate that our algorithm performs reliably on target organic shapes with minimal loss of input geometry.","1941-0506","","10.1109/TVCG.2023.3326595","King Abdullah University of Science and Technology (KAUST)(grant numbers:BAS/1/1680-01-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290929","Physicalization;Physical visualization;3D printing;Isometric decomposition;Direct remeshing;Biological structures;Intracellular compartments","Three-dimensional displays;Solid modeling;Shape;Biology;Biological system modeling;Biomembranes;Surface treatment","","","","53","CCBY","23 Oct 2023","","","IEEE","IEEE Journals"
"Extract and Characterize Hairpin Vortices in Turbulent Flows","A. Zafar; D. Yang; G. Chen","University of Houston, USA; University of Houston, USA; University of Houston, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","716","726","Hairpin vortices are one of the most important vortical structures in turbulent flows. Extracting and characterizing hairpin vortices provides useful insight into many behaviors in turbulent flows. However, hairpin vortices have complex configurations and might be entangled with other vortices, making their extraction difficult. In this work, we introduce a framework to extract and separate hairpin vortices in shear driven turbulent flows for their study. Our method first extracts general vortical regions with a region-growing strategy based on certain vortex criteria (e.g., $\lambda_{2}$) and then separates those vortices with the help of progressive extraction of ($\lambda_{2}$) iso-surfaces in a top-down fashion. This leads to a hierarchical tree representing the spatial proximity and merging relation of vortices. After separating individual vortices, their shape and orientation information is extracted. Candidate hairpin vortices are identified based on their shape and orientation information as well as their physical characteristics. An interactive visualization system is developed to aid the exploration, classification, and analysis of hairpin vortices based on their geometric and physical attributes. We also present additional use cases of the proposed system for the analysis and study of general vortices in other types of flows.","1941-0506","","10.1109/TVCG.2023.3326603","NSF OAC(grant numbers:2102761); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294258","Turbulent flow;vortices;hairpin vortex extraction","Visualization;Shape;Behavioral sciences;Histograms;Object recognition;Data mining;Training","","","","55","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations","R. Hnatyshyn; J. Zhao; D. Perez; J. Ahrens; R. Maciejewski","Arizona State University, USA; Arizona State University, USA; Los Alamos National Laboratory, USA; Los Alamos National Laboratory, USA; Arizona State University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","727","737","Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge physio-chemical research. They provide critical insights into how a physical system evolves over time given a model of interatomic interactions. Understanding a system's evolution is key to selecting the best candidates for new drugs, materials for manufacturing, and countless other practical applications. With today's technology, these simulations can encompass millions of unit transitions between discrete molecular structures, spanning up to several milliseconds of real time. Attempting to perform a brute-force analysis with data-sets of this size is not only computationally impractical, but would not shed light on the physically-relevant features of the data. Moreover, there is a need to analyze simulation ensembles in order to compare similar processes in differing environments. These problems call for an approach that is analytically transparent, computationally efficient, and flexible enough to handle the variety found in materials-based research. In order to address these problems, we introduce MolSieve, a progressive visual analytics system that enables the comparison of multiple long-duration simulations. Using MolSieve, analysts are able to quickly identify and compare regions of interest within immense simulations through its combination of control charts, data-reduction techniques, and highly informative visual components. A simple programming interface is provided which allows experts to fit MolSieve to their needs. To demonstrate the efficacy of our approach, we present two case studies of MolSieve and report on findings from domain collaborators.","1941-0506","","10.1109/TVCG.2023.3326584","Exascale Computing Project(grant numbers:17-SC-20-SC); US Department of Energy Office of Science; National Nuclear Security Administration; U.S. Department of Homeland Security(grant numbers:17STQAC00001-06-04); Los Alamos National Laboratory; Triad National Security LLC; National Nuclear Security administration; U.S. DOE(grant numbers:89233218CNA0000001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313127","Molecular dynamics;time-series analysis;visual analytics","Trajectory;Analytical models;Biological system modeling;Visual analytics;Three-dimensional displays;Computational modeling;Data models","","","","54","IEEE","8 Nov 2023","","","IEEE","IEEE Journals"
"PROWIS: A Visual Approach for Building, Managing, and Analyzing Weather Simulation Ensembles at Runtime","C. V. F. de Souza; S. M. Bonnet; D. de Oliveira; M. Cataldi; F. Miranda; M. Lage","Universidade Federal Fluminense and the University of Illinois, USA; Universidade Federal do Rio de Janeiro, Brazil; Universidade Federal Fluminense, Brazil; Universidade Federal Fluminense, Brazil; University of Illinois Chicago, USA; Universidade Federal Fluminense, Brazil","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","738","747","Weather forecasting is essential for decision-making and is usually performed using numerical modeling. Numerical weather models, in turn, are complex tools that require specialized training and laborious setup and are challenging even for weather experts. Moreover, weather simulations are data-intensive computations and may take hours to days to complete. When the simulation is finished, the experts face challenges analyzing its outputs, a large mass of spatiotemporal and multivariate data. From the simulation setup to the analysis of results, working with weather simulations involves several manual and error-prone steps. The complexity of the problem increases exponentially when the experts must deal with ensembles of simulations, a frequent task in their daily duties. To tackle these challenges, we propose ProWis: an interactive and provenance-oriented system to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. ProWis was built in close collaboration with weather experts, and we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.","1941-0506","","10.1109/TVCG.2023.3326514","CNPq(grant numbers:316963/2021-6); FAPERJ(grant numbers:E-26/202.915/2019,E-26/211.134/2019); CAPES(grant numbers:Finance Code 001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290915","Weather visualization;Ensemble visualization;Provenance management;WRF visual setup","Meteorology;Data visualization;Weather forecasting;Visualization;Atmospheric modeling;Runtime;Predictive models","","","","36","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"ViMO - Visual Analysis of Neuronal Connectivity Motifs","J. Troidl; S. Warchol; J. Choi; J. Matelsky; N. Dhanyasi; X. Wang; B. Wester; D. Wei; J. W. Lichtman; H. Pfister; J. Beyer","School of Engineering & Applied Sciences, Harvard University, USA; School of Engineering & Applied Sciences, Harvard University, USA; Department of Computer Science, Boston College, United States; Applied Physics Laboratory, Johns Hopkins University, USA; Department of Cellular & Molecular Biology, Harvard University, USA; Department of Cellular & Molecular Biology, Harvard University, USA; Applied Physics Laboratory, Johns Hopkins University, USA; Department of Computer Science, Boston College, United States; Department of Cellular & Molecular Biology, Harvard University, USA; School of Engineering & Applied Sciences, Harvard University, USA; School of Engineering & Applied Sciences, Harvard University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","748","758","Recent advances in high-resolution connectomics provide researchers with access to accurate petascale reconstructions of neuronal circuits and brain networks for the first time. Neuroscientists are analyzing these networks to better understand information processing in the brain. In particular, scientists are interested in identifying specific small network motifs, i.e., repeating subgraphs of the larger brain network that are believed to be neuronal building blocks. Although such motifs are typically small (e.g., 2–6 neurons), the vast data sizes and intricate data complexity present significant challenges to the search and analysis process. To analyze these motifs, it is crucial to review instances of a motif in the brain network and then map the graph structure to detailed 3D reconstructions of the involved neurons and synapses. We present Vimo, an interactive visual approach to analyze neuronal motifs and motif chains in large brain networks. Experts can sketch network motifs intuitively in a visual interface and specify structural properties of the involved neurons and synapses to query large connectomics datasets. Motif instances (MIs) can be explored in high-resolution 3D renderings. To simplify the analysis of MIs, we designed a continuous focus&context metaphor inspired by visual abstractions. This allows users to transition from a highly-detailed rendering of the anatomical structure to views that emphasize the underlying motif structure and synaptic connectivity. Furthermore, Vimo supports the identification of motif chains where a motif is used repeatedly (e.g., 2–4 times) to form a larger network structure. We evaluate Vimo in a user study and an in-depth case study with seven domain experts on motifs in a large connectome of the fruit fly, including more than 21,000 neurons and 20 million synapses. We find that Vimo enables hypothesis generation and confirmation through fast analysis iterations and connectivity highlighting.","1941-0506","","10.1109/TVCG.2023.3327388","NSF(grant numbers:IIS-1901030,NSF-IIS-2239688,NCS-FO-2124179,R24MH114785); Johns Hopkins University Applied Physics Laboratory's Independent Research & Development (IR&D) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298029","Visual motif analysis;Focus&Context;Scientific visualization;Neuroscience;Connectomics","Neurons;Visualization;Three-dimensional displays;Synapses;Morphology;Brain;Rendering (computer graphics)","","","","62","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Visual Analysis of Displacement Processes in Porous Media using Spatio-Temporal Flow Graphs","A. Straub; N. Karadimitriou; G. Reina; S. Frey; H. Steeb; T. Ertl","University of Stuttgart, Germany; University of Stuttgart, Germany; University of Stuttgart, Germany; University of Groningen, The Netherlands; University of Stuttgart, Germany; University of Stuttgart, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","759","769","We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media. We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow. To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map. From this map, we generate a spatio-temporal flow graph covering the whole process. This graph is further simplified to only reflect topological changes in the movement of the invading fluid. Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics. We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium. We finally report the generated insights from the domain experts and discuss our contribution's advantages, generality, and limitations.","1941-0506","","10.1109/TVCG.2023.3326931","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:327154368 - SFB 1313,EXC2075 - 390740016); Stuttgart Center for Simulation Science (SimTech); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296853","Comparative visualization;ensemble;graph;porous media","Fluids;Media;Visualization;Measurement;Solids;Geometry;Fingers","","","","61","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering","H. Jeon; G. J. Quadri; H. Lee; P. Rosen; D. A. Szafir; J. Seo","Seoul National University, South Korea; University of North Carolina, Chapel Hill, USA; UNIST, South Korea; University of Utah, USA; Seoul National University, South Korea; Seoul National University, South Korea","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","770","780","Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates the human-judged separability of two clusters. Then, CLAMS predicts cluster ambiguity by analyzing the aggregated results of all pairwise separability between clusters that are generated by the module. CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity. Meanwhile, CLAMS exhibits performance on par with human annotators. We conclude our work by presenting two applications for optimizing and benchmarking data mining techniques using CLAMS. The interactive demo of CLAMS is available at clusterambiguity.dev.","1941-0506","","10.1109/TVCG.2023.3327201","NAVER Corporation (Cloud Data Box); National Research Foundation of Korea (NRF)(grant numbers:2023R1A2C200520911); National Science Foundation(grant numbers:2127309); Computing Research Association(grant numbers:NSF IIS-2046725,NSF IIS-1764089,NSF IIS-2320920,NSF IIS-2233316,NSF III-2316496); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308641","Cluster;scatterplot;perception;cluster analysis;cluster ambiguity;visual quality measure","Visualization;Task analysis;Reliability;Benchmark testing;Data analysis;Complexity theory;Clustering algorithms","","","","86","CCBYNCND","3 Nov 2023","","","IEEE","IEEE Journals"
"Classes are Not Clusters: Improving Label-Based Evaluation of Dimensionality Reduction","H. Jeon; Y. -H. Kuo; M. Aupetit; K. -L. Ma; J. Seo","Seoul National University, South Korea; University of California, Davis, South Korea; Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar; University of California, Davis, South Korea; Seoul National University, South Korea","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","781","791","A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures—Label-Trustworthiness and Label-Continuity (Label-T&C)—advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.","1941-0506","","10.1109/TVCG.2023.3327187","National Research Foundation of Korea (NRF)(grant numbers:2023R1A2C200520911); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308618","Dimensionality Reduction;Reliability;Clustering;Clustering Validation Measures;Dimensionality Reduction Evaluation","Distortion measurement;Reliability;Dimensionality reduction;Nonlinear distortion;Extraterrestrial measurements;Scalability;Degradation","","","","74","CCBYNCND","3 Nov 2023","","","IEEE","IEEE Journals"
"Guaranteed Visibility in Scatterplots with Tolerance","L. Giovannangeli; F. Lalanne; R. Giot; R. Bourqui","Univ. Bordeaux, CNRS, Bordeaux INP, INRIA, LaBRI, UMR 5800, Talence, France; Univ. Bordeaux, CNRS, Bordeaux INP, INRIA, LaBRI, UMR 5800, Talence, France; Univ. Bordeaux, CNRS, Bordeaux INP, INRIA, LaBRI, UMR 5800, Talence, France; Univ. Bordeaux, CNRS, Bordeaux INP, INRIA, LaBRI, UMR 5800, Talence, France","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","792","802","In 2D visualizations, visibility of every datum's representation is crucial to ease the completion of visual tasks. Such a guarantee is barely respected in complex visualizations, mainly because of overdraws between datum representations that hide parts of the information (e.g., outliers). The literature proposes various Layout Adjustment algorithms to improve the readability of visualizations that suffer from this issue. Manipulating the data in high-dimensional, geometric or visual space; they rely on different strategies with their own strengths and weaknesses. Moreover, most of these algorithms are computationally expensive as they search for an exact solution in the geometric space and do not scale well to large datasets. This article proposes GIST, a layout adjustment algorithm that aims at optimizing three criteria: (i) node visibility guarantee (at least 1 pixel), (ii) node size maximization, and (iii) the original layout preservation. This is achieved by combining a search for the maximum node size that enables to draw all the data points without overlaps, with a limited budget of movements (i.e., limiting the distortions of the original layout). The method's basis relies on the idea that it is not necessary for two data representations to be strictly not overlapping in order to guarantee their visibility in visual space. Our algorithm therefore uses a tolerance in the geometric space to determine the overlaps between pairs of data. The tolerance is optimized such that the approximation computed in the geometric space can lead to visualization without noticeable overdraw after the data rendering rasterization. In addition, such an approximation helps to ease the algorithm's convergence as it reduces the number of constraints to resolve, enabling it to handle large datasets. We demonstrate the effectiveness of our approach by comparing its results to those of state-of-the-art methods on several large datasets.","1941-0506","","10.1109/TVCG.2023.3326596","ANR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292566","Guaranteed visibility;Layout adjustment;Overlap removal;Scatterplots","Layout;Data visualization;Visualization;Approximation algorithms;Task analysis;Dispersion;Stress","","","","40","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Data Navigator: An Accessibility-Centered Data Navigation Toolkit","F. Elavsky; L. Nadolskis; D. Moritz","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","803","813","Making data visualizations accessible for people with disabilities remains a significant challenge in current practitioner efforts. Existing visualizations often lack an underlying navigable structure, fail to engage necessary input modalities, and rely heavily on visual-only rendering practices. These limitations exclude people with disabilities, especially users of assistive technologies. To address these challenges, we present Data Navigator: a system built on a dynamic graph structure, enabling developers to construct navigable lists, trees, graphs, and flows as well as spatial, diagrammatic, and geographic relations. Data Navigator supports a wide range of input modalities: screen reader, keyboard, speech, gesture detection, and even fabricated assistive devices. We present 3 case examples with Data Navigator, demonstrating we can provide accessible navigation structures on top of raster images, integrate with existing toolkits at scale, and rapidly develop novel prototypes. Data Navigator is a step towards making accessible data visualizations easier to design and implement.","1941-0506","","10.1109/TVCG.2023.3327393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301522","accessibility;visualization;tools;technical materials;platforms;data interaction","Data visualization;Navigation;Visualization;Rendering (computer graphics);Semantics;Vegetation;Guidelines","","1","","47","IEEE","30 Oct 2023","","","IEEE","IEEE Journals"
"NL2Color: Refining Color Palettes for Charts with Natural Language","C. Shi; W. Cui; C. Liu; C. Zheng; H. Zhang; Q. Luo; X. Ma","Southeast University, China; Microsoft Research Asia, China; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China; Microsoft Research Asia, China; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","814","824","Choice of color is critical to creating effective charts with an engaging, enjoyable, and informative reading experience. However, designing a good color palette for a chart is a challenging task for novice users who lack related design expertise. For example, they often find it difficult to articulate their abstract intentions and translate these intentions into effective editing actions to achieve a desired outcome. In this work, we present NL2Color, a tool that allows novice users to refine chart color palettes using natural language expressions of their desired outcomes. We first collected and categorized a dataset of 131 triplets, each consisting of an original color palette of a chart, an editing intent, and a new color palette designed by human experts according to the intent. Our tool employs a large language model (LLM) to substitute the colors in original palettes and produce new color palettes by selecting some of the triplets as few-shot prompts. To evaluate our tool, we conducted a comprehensive two-stage evaluation, including a crowd-sourcing study ($\mathrm{N}=71$) and a within-subjects user study ($\mathrm{N}=12$). The results indicate that the quality of the color palettes revised by NL2Color has no significantly large difference from those designed by human experts. The participants who used NL2Color obtained revised color palettes to their satisfaction in a shorter period and with less effort.","1941-0506","","10.1109/TVCG.2023.3326522","HKUST 30 for 30(grant numbers:3030_003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292693","chart;color palette;natural language;large language model","Image color analysis;Color;Natural languages;Data visualization;Task analysis;Refining;Bars","","","","57","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Reducing Ambiguities in Line-Based Density Plots by Image-Space Colorization","Y. Xue; P. Paetzold; R. Kehlbeck; B. Chen; K. C. Kwan; Y. Wang; O. Deussen","University of Konstanz, Germany and Shandong University, China; University of Konstanz, Germany; University of Konstanz, Germany; University of Konstanz, Germany; California State University Sacramento, United States; Shandong University, China; University of Konstanz, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","825","835","Line-based density plots are used to reduce visual clutter in line charts with a multitude of individual lines. However, these traditional density plots are often perceived ambiguously, which obstructs the user's identification of underlying trends in complex datasets. Thus, we propose a novel image space coloring method for line-based density plots that enhances their interpretability. Our method employs color not only to visually communicate data density but also to highlight similar regions in the plot, allowing users to identify and distinguish trends easily. We achieve this by performing hierarchical clustering based on the lines passing through each region and mapping the identified clusters to the hue circle using circular MDS. Additionally, we propose a heuristic approach to assign each line to the most probable cluster, enabling users to analyze density and individual lines. We motivate our method by conducting a small-scale user study, demonstrating the effectiveness of our method using synthetic and real-world datasets, and providing an interactive online tool for generating colored line-based density plots.","1941-0506","","10.1109/TVCG.2023.3327149","Deutsche Forschungsgemeinschaft (DFG)(grant numbers:Project 410883423,Project 251654672 - TRR 161,KE 740/17-2); National Key R&D Program of China(grant numbers:2022ZD0160805); NSF(grant numbers:62132017,62141217); Shandong Provincial Natural Science Foundation(grant numbers:ZQ2022JQ32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297597","Trajectory data;times series;density-based visualization;clustering;coloring","Visualization;Image color analysis;Market research;Trajectory;Clutter","","","","83","CCBYNCND","26 Oct 2023","","","IEEE","IEEE Journals"
"TactualPlot: Spatializing Data as Sound Using Sensory Substitution for Touchscreen Accessibility","P. Chundury; Y. Reyazuddin; J. B. Jordan; J. Lazar; N. Elmqvist","University of Maryland, College Park, College Park, MD, USA; National Federation of the Blind, Baltimore, MD, USA; University of Maryland, College Park, College Park, MD, USA; University of Maryland, College Park, College Park, MD, USA; Aarhus University, Aarhus, Denmark","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","836","846","Tactile graphics are one of the best ways for a blind person to perceive a chart using touch, but their fabrication is often costly, time-consuming, and does not lend itself to dynamic exploration. Refreshable haptic displays tend to be expensive and thus unavailable to most blind individuals. We propose TactualPlot, an approach to sensory substitution where touch interaction yields auditory (sonified) feedback. The technique relies on embodied cognition for spatial awareness—i.e., individuals can perceive 2D touch locations of their fingers with reference to other 2D locations such as the relative locations of other fingers or chart characteristics that are visualized on touchscreens. Combining touch and sound in this way yields a scalable data exploration method for scatterplots where the data density under the user's fingertips is sampled. The sample regions can optionally be scaled based on how quickly the user moves their hand. Our development of TactualPlot was informed by formative design sessions with a blind collaborator, whose practice while using tactile scatterplots caused us to expand the technique for multiple fingers. We present results from an evaluation comparing our TactualPlot interaction technique to tactile graphics printed on swell touch paper.","1941-0506","","10.1109/TVCG.2023.3326937","U.S. National Science Foundation(grant numbers:IIS-2211628); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308620","Accessibility;sonification;multimodal interaction;crossmodal interaction;visualization","Sonification;Data visualization;Smart phones;Touch sensitive screens;Blindness;Haptic interfaces;Electronic mail","","","","61","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"FSLens: A Visual Analytics Approach to Evaluating and Optimizing the Spatial Layout of Fire Stations","L. Chen; H. Wang; Y. Ouyang; Y. Zhou; N. Wang; Q. Li","School of Information Science and Technology, ShanghaiTech University, China; School of Information Science and Technology, ShanghaiTech University, China; School of Information Science and Technology, ShanghaiTech University, China; College of Civil Engineering and Architecture, Zhejiang University, China; College of Civil Engineering and Architecture, Zhejiang University, China; School of Information Science and Technology, ShanghaiTech University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","847","857","The provision of fire services plays a vital role in ensuring the safety of residents' lives and property. The spatial layout of fire stations is closely linked to the efficiency of fire rescue operations. Traditional approaches have primarily relied on mathematical planning models to generate appropriate layouts by summarizing relevant evaluation criteria. However, this optimization process presents significant challenges due to the extensive decision space, inherent conflicts among criteria, and decision-makers' preferences. To address these challenges, we propose FSLens, an interactive visual analytics system that enables in-depth evaluation and rational optimization of fire station layout. Our approach integrates fire records and correlation features to reveal fire occurrence patterns and influencing factors using spatiotemporal sequence forecasting. We design an interactive visualization method to explore areas within the city that are potentially under-resourced for fire service based on the fire distribution and existing fire station layout. Moreover, we develop a collaborative human-computer multi-criteria decision model that generates multiple candidate solutions for optimizing firefighting resources within these areas. We simulate and compare the impact of different solutions on the original layout through well-designed visualizations, providing decision-makers with the most satisfactory solution. We demonstrate the effectiveness of our approach through one case study with real-world datasets. The feedback from domain experts indicates that our system helps them to better identify and improve potential gaps in the current fire station layout.","1941-0506","","10.1109/TVCG.2023.3327077","Research and System Development of Resilient Urban Fire Risk and Fire Safety Dynamic Assessment Technology Based on Multiple Data Sources(grant numbers:2020XFZD19); ShanghaiTech University; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295565","Spatiotemporal Analysis;Multi-criteria Decision Making;Visualization","Layout;Optimization;Time factors;Biological system modeling;Planning;Mathematical models;MCDM","","","","69","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization","Y. Fu; J. Stasko","Georgia Institute of Technology, USA; Georgia Institute of Technology, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","858","868","Data visualization has the power to revolutionize sports. For example, the rise of shot maps has changed basketball strategy by visually illustrating where “good/bad” shots are taken from. As a result, professional basketball teams today take shots from very different positions on the court than they did 20 years ago. Although the shot map has transformed many facets of the game, there is still much room for improvement to support richer and more complex analytical tasks. More specifically, we believe that the lack of sufficient interactivity to support various analytical queries and the inability to visually compare differences across situations are significant limitations of current shot maps. To address these limitations and showcase new possibilities, we designed and developed HoopInSight, an interactive visualization system that centers around a novel spatial comparison visual technique, enhancing the capabilities of shot maps in basketball analytics. This article presents the system, with a focus on our proposed visual technique and its accompanying interactions, all designed to promote comparison of two different scenarios. Furthermore, we provide reflections on and a discussion of relevant issues, including considerations for designing spatial comparison techniques, the scalability and transferability of this approach, and the benefits and pitfalls of designing as domain experts.","1941-0506","","10.1109/TVCG.2023.3326910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292892","sports data visualization;sports analytics;visual comparison;basketball","Data visualization;Sports;Visualization;Games;Fans;Task analysis;Stakeholders","","","","52","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"SkiVis: Visual Exploration and Route Planning in Ski Resorts","J. Rauscher; R. Buchmüller; D. A. Keim; M. Miller","University of Konstanz, Germany; University of Konstanz, Germany; University of Konstanz, Germany; University of Konstanz, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","869","879","Optimal ski route selection is a challenge based on a multitude of factors, such as the steepness, compass direction, or crowdedness. The personal preferences of every skier towards these factors require individual adaptations, which aggravate this task. Current approaches within this domain do not combine automated routing capabilities with user preferences, missing out on the possibility of integrating domain knowledge in the analysis process. We introduce SkiVis, a visual analytics application to interactively explore ski slopes and provide routing recommendations based on user preferences. In collaboration with ski guides and enthusiasts, we elicited requirements and guidelines for such an application and propose different workflows depending on the skiers' familiarity with the resort. In a case study on the resort of Ski Arlberg, we illustrate how to leverage volunteered geographic information to enable a numerical comparison between slopes. We evaluated our approach through a pair-analytics study and demonstrate how it supports skiers in discovering relevant and preference-based ski routes. Besides the tasks investigated in the study, we derive additional use cases from the interviews that showcase the further potential of SkiVis, and contribute directions for further research opportunities.","1941-0506","","10.1109/TVCG.2023.3326940","Deutsche Forschungsgemeinschaft (DFG)(grant numbers:455910360); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294204","Geographic Visualization;Routing","Routing;Snow;Planning;Task analysis;Geospatial analysis;Navigation;Compass","","","","88","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Action-Evaluator: A Visualization Approach for Player Action Evaluation in Soccer","A. Cao; X. Xie; M. Zhou; H. Zhang; M. Xu; Y. Wu","State Key Lab of CAD&CG, Zhejiang University, China; Department of Sports Science, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; Department of Sports Science, Zhejiang University, China; Ministry of Education, School of Computer and Artificial Intelligence, Zhengzhou University, Engineering Research Center of Intelligent Swarm Systems, National Supercomputing Center, Zhengzhou, China; State Key Lab of CAD&CG, Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","880","890","In soccer, player action evaluation provides a fine-grained method to analyze player performance and plays an important role in improving winning chances in future matches. However, previous studies on action evaluation only provide a score for each action, and hardly support inspecting and comparing player actions integrated with complex match context information such as team tactics and player locations. In this work, we collaborate with soccer analysts and coaches to characterize the domain problems of evaluating player performance based on action scores. We design a tailored visualization of soccer player actions that places the action choice together with the tactic it belongs to as well as the player locations in the same view. Based on the design, we introduce a visual analytics system, Action-Evaluator, to facilitate a comprehensive player action evaluation through player navigation, action investigation, and action explanation. With the system, analysts can find players to be analyzed efficiently, learn how they performed under various match situations, and obtain valuable insights to improve their action choices. The usefulness and effectiveness of this work are demonstrated by two case studies on a real-world dataset and an expert interview.","1941-0506","","10.1109/TVCG.2023.3326524","National Key R&D Program of China(grant numbers:2022YFE0137800); NSFC(grant numbers:U22A2032,62202424); Collaborative Innovation Center of Artificial Intelligence; MOE; Zhejiang Provincial Government (ZJU); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296094","Soccer Visualization;Player Evaluation;Design Study","Sports;Data visualization;Task analysis;Visual analytics;Performance evaluation;Pattern matching;Behavioral sciences","Soccer;Athletic Performance;Computer Graphics","","","65","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"TransforLearn: Interactive Visual Tutorial for the Transformer Model","L. Gao; Z. Shao; Z. Luo; H. Hu; C. Turkay; S. Chen","School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; School of Data Science, Fudan University, China; Chongqing University, China; University of Warwick, United Kingdom; School of Data Science, Fudan University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","891","901","The widespread adoption of Transformers in deep learning, serving as the core framework for numerous large-scale language models, has sparked significant interest in understanding their underlying mechanisms. However, beginners face difficulties in comprehending and learning Transformers due to its complex structure and abstract data representation. We present TransforLearn, the first interactive visual tutorial designed for deep learning beginners and non-experts to comprehensively learn about Transformers. TransforLearn supports interactions for architecture-driven exploration and task-driven exploration, providing insight into different levels of model details and their working processes. It accommodates interactive views of each layer's operation and mathematical formula, helping users to understand the data flow of long text sequences. By altering the current decoder-based recursive prediction results and combining the downstream task abstractions, users can deeply explore model processes. Our user study revealed that the interactions of TransforLearn are positively received. We observe that TransforLearn facilitates users' accomplishment of study tasks and a grasp of key concepts in Transformer effectively.","1941-0506","","10.1109/TVCG.2023.3327353","Natural Science Foundation of China (NSFC)(grant numbers:62202105); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0103); General Program(grant numbers:21ZR1403300); Sailing Program(grant numbers:21YF1402900); ZJLab; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297598","Deep learning;Transformer;Visual tutorial;Explorable explanations","Transformers;Task analysis;Visualization;Deep learning;Tutorials;Mathematical models;Data visualization","","","","62","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization","D. Atzberger; T. Cech; M. Trapp; R. Richter; W. Scheibel; J. Döllner; T. Schreck","Digital Engineering Faculty, University of Potsdam, Hasso Plattner Institute, Germany; Digital Engineering Faculty, University of Potsdam, Germany; Digital Engineering Faculty, University of Potsdam, Hasso Plattner Institute, Germany; Digital Engineering Faculty, University of Potsdam, Germany; Digital Engineering Faculty, University of Potsdam, Hasso Plattner Institute, Germany; Digital Engineering Faculty, University of Potsdam, Germany; Graz University of Technology, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","902","912","Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.","1941-0506","","10.1109/TVCG.2023.3326569","Federal Ministry of Education and Research, Germany(grant numbers:01IS22062,01IS20088B); Federal Ministry for Economic Affairs and Climate Action of Germany(grant numbers:16KN086467); Austrian Research Promotion Agency (FFG)(grant numbers:FO999899544); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290971","Text visualization;spatialization;dimensionality reduction algorithms;topic modeling","Dimensionality reduction;Measurement;Visualization;Analytical models;Computational modeling;Layout;Semantics","","","","81","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"A Computational Design Pipeline to Fabricate Sensing Network Physicalizations","S. S. Bae; T. Fujiwara; A. Ynnerman; E. Y. -L. Do; M. L. Rivera; D. A. Szafir","University of Colorado, Boulder, United States; Linköping University, Sweden; Linköping University, Sweden; University of Colorado, Boulder, United States; University of Colorado, Boulder, United States; University of North Carolina-Chapel Hill, United States","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","913","923","Interaction is critical for data analysis and sensemaking. However, designing interactive physicalizations is challenging as it requires cross-disciplinary knowledge in visualization, fabrication, and electronics. Interactive physicalizations are typically produced in an unstructured manner, resulting in unique solutions for a specific dataset, problem, or interaction that cannot be easily extended or adapted to new scenarios or future physicalizations. To mitigate these challenges, we introduce a computational design pipeline to 3D print network physicalizations with integrated sensing capabilities. Networks are ubiquitous, yet their complex geometry also requires significant engineering considerations to provide intuitive, effective interactions for exploration. Using our pipeline, designers can readily produce network physicalizations supporting selection—the most critical atomic operation for interaction—by touch through capacitive sensing and computational inference. Our computational design pipeline introduces a new design paradigm by concurrently considering the form and interactivity of a physicalization into one cohesive fabrication workflow. We evaluate our approach using (i) computational evaluations, (ii) three usage scenarios focusing on general visualization tasks, and (iii) expert interviews. The design paradigm introduced by our pipeline can lower barriers to physicalization research, creation, and adoption.","1941-0506","","10.1109/TVCG.2023.3327198","U.S. National Science Foundation(grant numbers:IIS-2040489,IIS-1764089,IIS-2320920,IIS-1933915,IIS-2233316); Knut and Alice Wallenberg Foundation(grant numbers:KAW 2019.0024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10303764","Physicalization;tangible interfaces;3D printing;computational fabrication;design automation;network data","Sensors;Pipelines;Fabrication;Three-dimensional displays;Data visualization;Three-dimensional printing;Task analysis","","","","94","IEEE","31 Oct 2023","","","IEEE","IEEE Journals"
"Designing for Ambiguity in Visual Analytics: Lessons from Risk Assessment and Prediction","S. Nowak; L. Bartram","Simon Fraser University, Canada; Simon Fraser University, Canada","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","924","933","Ambiguity is pervasive in the complex sensemaking domains of risk assessment and prediction but there remains little research on how to design visual analytics tools to accommodate it. We report on findings from a qualitative study based on a conceptual framework of sensemaking processes to investigate how both new visual analytics designs and existing tools, primarily data tables, support the cognitive work demanded in avalanche forecasting. While both systems yielded similar analytic outcomes we observed differences in ambiguous sensemaking and the analytic actions either afforded. Our findings challenge conventional visualization design guidance in both perceptual and interaction design, highlighting the need for data interfaces that encourage reflection, provoke alternative interpretations, and support the inherently ambiguous nature of sensemaking in this critical application. We review how different visual and interactive forms support or impede analytic processes and introduce “gisting” as a significant yet unexplored analytic action for visual analytics research. We conclude with design implications for enabling ambiguity in visual analytics tools to scaffold sensemaking in risk assessment.","1941-0506","","10.1109/TVCG.2023.3326571","Mitacs; Natural Sciences and Engineering Research Council Industry Research Chair(grant numbers:IRC/5155322016); Canadian Pacific Railway; HeliCat Canada; Canadian Avalanche Association; Mike Wiegele Helicopter Skiing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291029","Complex Systems;Risk Assessment;Sensemaking;Visualization Design","Visual analytics;Forecasting;Hazards;Data visualization;Risk management;Uncertainty;Task analysis","","","","66","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization","H. Kim; R. Rossi; J. Hullman; J. Hoffswell","Northwestern University, USA; Adobe Research, USA; Northwestern University, USA; Adobe Research, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","934","943","Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.","1941-0506","","10.1109/TVCG.2023.3326583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290918","Visualization;responsive visualization;mixed-initiative authoring","Data visualization;Visualization;Pipelines;Bars;Automation;Authoring systems;Annotations","","","","45","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks","Y. Lin; H. Li; L. Yang; A. Wu; H. Qu","Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China; Hong Kong University of Science and Technology, China; Harvard University, USA; Hong Kong University of Science and Technology, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","944","954","Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.","1941-0506","","10.1109/TVCG.2023.3327170","HK RGC GRF(grant numbers:16210722); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296056","Computational Notebook;Sketch-based Interaction;Documentation;Visualization;Exploratory Data Analysis","Data visualization;Documentation;Data analysis;Visualization;Codes;Natural languages;Data models","","","","58","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Why Change My Design: Explaining Poorly Constructed Visualization Designs with Explorable Explanations","L. Y. -H. Lo; Y. Cao; L. Yang; H. Qu","Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","955","964","Although visualization tools are widely available and accessible, not everyone knows the best practices and guidelines for creating accurate and honest visual representations of data. Numerous books and articles have been written to expose the misleading potential of poorly constructed charts and teach people how to avoid being deceived by them or making their own mistakes. These readings use various rhetorical devices to explain the concepts to their readers. In our analysis of a collection of books, online materials, and a design workshop, we identified six common explanation methods. To assess the effectiveness of these methods, we conducted two crowdsourced studies (each with $N=125$) to evaluate their ability to teach and persuade people to make design changes. In addition to these existing methods, we brought in the idea of Explorable Explanations, which allows readers to experiment with different chart settings and observe how the changes are reflected in the visualization. While we did not find significant differences across explanation methods, the results of our experiments indicate that, following the exposure to the explanations, the participants showed improved proficiency in identifying deceptive charts and were more receptive to proposed alterations of the visualization design. We discovered that participants were willing to accept more than 60% of the proposed adjustments in the persuasiveness assessment. Nevertheless, we found no significant differences among different explanation methods in convincing participants to accept the modifications.","1941-0506","","10.1109/TVCG.2023.3327155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299537","Information Visualization;Deceptive Visualization;Explorable Explanations","Data visualization;Visualization;Best practices;Fake news;Social networking (online);Guidelines;Education","","","","40","IEEE","27 Oct 2023","","","IEEE","IEEE Journals"
"Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization","S. W. Wurster; T. Xiong; H. -W. Shen; H. Guo; T. Peterka","The Ohio State University, USA; The Ohio State University, USA; The Ohio State University, USA; The Ohio State University, USA; The Ohio State University, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","965","974","Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.","1941-0506","","10.1109/TVCG.2023.3327194","US Department of Energy SciDAC program(grant numbers:DE-SC0021360); National Science Foundation Division of Information and Intelligent Systems(grant numbers:IIS-1955764); National Science Foundation Office of Advanced Cyberinfrastructure(grant numbers:OAC-2112606,OAC-2311878); Advanced Scientific Computing Research, Office of Science; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357,DE-SC0022753,DE-SC0023193); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297599","Scene representation network;deep learning;scientific visualization;volume rendering","Training;Adaptation models;Data models;Solid modeling;Computer architecture;Rendering (computer graphics);Encoding","","1","","35","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination","D. Bauer; Q. Wu; K. -L. Ma","University of California, Davis, USA; University of California, Davis, USA; University of California, Davis, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","975","985","Volume data is commonly found in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks—a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achieve interactive framerates even on large datasets. We conduct in-depth evaluations of the method's performance, including visual quality, stochastic noise, inference and rendering speeds, and accuracy regarding illumination and phase function awareness. Results are compared to ray marching, path tracing and photon mapping. Our findings show that Photon Field Networks can faithfully represent indirect global illumination within the boundaries of the trained phase spectrum while exhibiting less stochastic noise and rendering at a significantly faster rate than traditional methods.","1941-0506","","10.1109/TVCG.2023.3327107","U.S. Department of Energy(grant numbers:DE-SC0019486); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297590","Volume data;volume rendering;volume visualization;deep learning;global illumination;neural rendering;path tracing","Photonics;Rendering (computer graphics);Lighting;Data visualization;Real-time systems;Light sources;Visualization","","","","65","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data","J. Shen; H. -W. Shen","Department of Computer Science and Engineering, The Ohio State University, United States; Department of Computer Science and Engineering, The Ohio State University, United States","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","986","996","Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification for the super-resolved results. During model training, we augment the training data with samples across various scales to make the model adaptable to data of different scales, achieving flexible super-resolution for a given input. Our results demonstrate superior performance and robust uncertainty quantification compared with existing methods such as interpolation and GAN-based super-resolution networks.","1941-0506","","10.1109/TVCG.2023.3327171","US Department of Energy SciDAC(grant numbers:DE-SC0021360,DE-SC0023193); National Science Foundation(grant numbers:IIS-1955764); Los Alamos National Laboratory Contract(grant numbers:C3435); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309865","Super resolution;latent space;normalizing flow;uncertainty visualization","Superresolution;Uncertainty;Data models;Data visualization;Gaussian distribution;Probabilistic logic;Computational modeling","","","","42","IEEE","6 Nov 2023","","","IEEE","IEEE Journals"
"A Heuristic Approach for Dual Expert/End-User Evaluation of Guidance in Visual Analytics","D. Ceneda; C. Collins; M. El-Assady; S. Miksch; C. Tominski; A. Arleo","TU Wien, Austria; Ontario Tech University, Canada; ETH Zurich, AI Center, Switzerland; TU Wien, Austria; VAC Institute, University of Rostock, Germany; TU Wien, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","997","1007","Guidance can support users during the exploration and analysis of complex data. Previous research focused on characterizing the theoretical aspects of guidance in visual analytics and implementing guidance in different scenarios. However, the evaluation of guidance-enhanced visual analytics solutions remains an open research question. We tackle this question by introducing and validating a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and collect expert feedback on their validity. To facilitate actual evaluation studies, we derive two sets of heuristics. The first set targets heuristic evaluations conducted by expert evaluators. The second set facilitates end-user studies where participants actually use a guidance-enhanced system. By following such a dual approach, the different quality criteria of guidance can be examined from two different perspectives, enhancing the overall value of evaluation studies. To test the practical utility of our methodology, we employ it in two studies to gain insight into the quality of two guidance-enhanced visual analytics solutions, one being a work-in-progress research prototype, and the other being a publicly available visualization recommender system. Based on these two evaluations, we derive good practices for conducting evaluations of guidance in visual analytics and identify pitfalls to be avoided during such studies.","1941-0506","","10.1109/TVCG.2023.3327152","Vienna Science and Technology Fund (WWTF)(grant numbers:10.47379/ICT19047); DoRIAH (FFG)(grant numbers:880883); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301693","Guidance;heuristics;evaluation;visual analytics","Visual analytics;Task analysis;Data visualization;Usability;Recommender systems;Navigation;Adaptive systems","","","","55","CCBYNCND","30 Oct 2023","","","IEEE","IEEE Journals"
"The Arrangement of Marks Impacts Afforded Messages: Ordering, Partitioning, Spacing, and Coloring in Bar Charts","R. Fygenson; S. Franconeri; E. Bertini","Northeastern University, USA; Northeastern University, USA; Northeastern University, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","1008","1018","Data visualizations present a massive number of potential messages to an observer. One might notice that one group's average is larger than another's, or that a difference in values is smaller than a difference between two others, or any of a combinatorial explosion of other possibilities. The message that a viewer tends to notice – the message that a visualization ‘affords’ – is strongly affected by how values are arranged in a chart, e.g., how the values are colored or positioned. Although understanding the mapping between a chart's arrangement and what viewers tend to notice is critical for creating guidelines and recommendation systems, current empirical work is insufficient to lay out clear rules. We present a set of empirical evaluations of how different messages-including ranking, grouping, and part-to-whole relationships–are afforded by variations in ordering, partitioning, spacing, and coloring of values, within the ubiquitous case study of bar graphs. In doing so, we introduce a quantitative method that is easily scalable, reviewable, and replicable, laying groundwork for further investigation of the effects of arrangement on message affordances across other visualizations and tasks. Pre-registration and all supplemental materials are available at https://osf.io/np3q7 and https://osf.io/bvy95, respectively.","1941-0506","","10.1109/TVCG.2023.3326590","National Science Foundation(grant numbers:2236644); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10291030","Perception & cognition;Methodologies;Human-subjects qualitative studies;Human-subjects quantitative studies;Charts;diagrams and plots;General public","Bars;Visualization;Data visualization;Task analysis;Affordances;COVID-19;Birds","","","","67","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Design Characterization for Black-and-White Textures in Visualization","T. He; Y. Zhong; P. Isenberg; T. Isenberg","Université Paris-Saclay, CNRS, Inria, LISN, France; Tencent Technology (Shenzhen) Company Limited, China; Université Paris-Saclay, CNRS, Inria, LISN, France; Université Paris-Saclay, CNRS, Inria, LISN, France","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1019","1029","We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures. Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings. We specifically study how to use what we call geometric and iconic textures. Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories. We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters. 30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps. We then had 150 participants rate these designs for aesthetics. Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.","1941-0506","","10.1109/TVCG.2023.3326941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298040","Aesthetics;textures;icons;black and white;visualization;visual representations;categorical data;design;perception","Data visualization;Visualization;Shape;Image color analysis;Bars;Encoding;Electronic mail","","","","53","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Image or Information? Examining the Nature and Impact of Visualization Perceptual Classification","A. Arunkumar; L. Padilla; G. -Y. Bae; C. Bryan","Arizona State University, USA; Northeastern University, USA; Arizona State University, USA; Arizona State University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1030","1040","How do people internalize visualizations: as images or information? In this study, we investigate the nature of internalization for visualizations (i.e., how the mind encodes visualizations in memory) and how memory encoding affects its retrieval. This exploratory work examines the influence of various design elements on a user's perception of a chart. Specifically, which design elements lead to perceptions of visualization as an image (aims to provide visual references, evoke emotions, express creativity, and inspire philosophic thought) or as information (aims to present complex data, information, or ideas concisely and promote analytical thinking)? Understanding how design elements contribute to viewers perceiving a visualization more as an image or information will help designers decide which elements to include to achieve their communication goals. For this study, we annotated 500 visualizations and analyzed the responses of 250 online participants, who rated the visualizations on a bilinear scale as ‘image’ or ‘information.’ We then conducted an in-person study ($n = 101$) using a free recall task to examine how the image/information ratings and design elements impacted memory. The results revealed several interesting findings: Image-rated visualizations were perceived as more aesthetically ‘appealing,’ ‘enjoyable,’ and ‘pleasing.’ Information-rated visualizations were perceived as less ‘difficult to understand’ and more aesthetically ‘likable’ and ‘nice,’ though participants expressed higher ‘positive’ sentiment when viewing image-rated visualizations and felt less ‘guided to a conclusion.’ The presence of axes and text annotations heavily influenced the likelihood of participants rating the visualization as ‘information.’ We also found different patterns among participants that were older. Importantly, we show that visualizations internalized as ‘images’ are less effective in conveying trends and messages, though they elicit a more positive emotional judgment, while ‘informative’ visualizations exhibit annotation focused recall and elicit a more positive design judgment. We discuss the implications of this dissociation between aesthetic pleasure and perceived ease of use in visualization design.","1941-0506","","10.1109/TVCG.2023.3326919","U.S. National Science Foundation(grant numbers:DUE-2216452,IIS-2238175); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294209","Information Visualization;Human-Centered Computing;Perception & Cognition;Takeaways","Data visualization;Visualization;Task analysis;Economics;Sports;Meteorology;Geology","Humans;Computer Graphics;Mental Recall;Communication;Judgment","","","84","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Perception of Line Attributes for Visualization","A. Sterzik; N. Lichtenberg; J. Wilms; M. Krone; D. W. Cunningham; K. Lawonn","University of Jena, Germany; University of Tübingen, Germany; University of Jena, Germany; University of Tübingen, Germany; Brandenburg University of Technology, Germany; University of Jena, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1041","1051","Line attributes such as width and dashing are commonly used to encode information. However, many questions on the perception of line attributes remain, such as how many levels of attribute variation can be distinguished or which line attributes are the preferred choices for which tasks. We conducted three studies to develop guidelines for using stylized lines to encode scalar data. In our first study, participants drew stylized lines to encode uncertainty information. Uncertainty is usually visualized alongside other data. Therefore, alternative visual channels are important for the visualization of uncertainty. Additionally, uncertainty—e.g., in weather forecasts—is a familiar topic to most people. Thus, we picked it for our visualization scenarios in study 1. We used the results of our study to determine the most common line attributes for drawing uncertainty: Dashing, luminance, wave amplitude, and width. While those line attributes were especially common for drawing uncertainty, they are also commonly used in other areas. In studies 2 and 3, we investigated the discriminability of the line attributes determined in study 1. Studies 2 and 3 did not require specific application areas; thus, their results apply to visualizing any scalar data in line attributes. We evaluated the just-noticeable differences (JND) and derived recommendations for perceptually distinct line levels. We found that participants could discriminate considerably more levels for the line attribute width than for wave amplitude, dashing, or luminance.","1941-0506","","10.1109/TVCG.2023.3326523","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:437702916); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292709","Line Drawings;Line Stylization;Perceptual Evaluation;Uncertainty Visualization","Uncertainty;Data visualization;Visualization;Encoding;Task analysis;Shape;Image color analysis","","1","","48","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Perceptually Uniform Construction of Illustrative Textures","A. Sterzik; M. Meuschke; D. W. Cunningham; K. Lawonn","University of Jena, Germany; University of Magdeburg, Germany; Brandenburg University of Technology, Germany; University of Jena, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1052","1062","Illustrative textures, such as stippling or hatching, were predominantly used as an alternative to conventional Phong rendering. Recently, the potential of encoding information on surfaces or maps using different densities has also been recognized. This has the significant advantage that additional color can be used as another visual channel and the illustrative textures can then be overlaid. Effectively, it is thus possible to display multiple information, such as two different scalar fields on surfaces simultaneously. In previous work, these textures were manually generated and the choice of density was unempirically determined. Here, we first want to determine and understand the perceptual space of illustrative textures. We chose a succession of simplices with increasing dimensions as primitives for our textures: Dots, lines, and triangles. Thus, we explore the texture types of stippling, hatching, and triangles. We create a range of textures by sampling the density space uniformly. Then, we conduct three perceptual studies in which the participants performed pairwise comparisons for each texture type. We use multidimensional scaling (MDS) to analyze the perceptual spaces per category. The perception of stippling and triangles seems relatively similar. Both are adequately described by a 1D manifold in 2D space. The perceptual space of hatching consists of two main clusters: Crosshatched textures, and textures with only one hatching direction. However, the perception of hatching textures with only one hatching direction is similar to the perception of stippling and triangles. Based on our findings, we construct perceptually uniform illustrative textures. Afterwards, we provide concrete application examples for the constructed textures.","1941-0506","","10.1109/TVCG.2023.3326574","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:437702916); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292705","Illustrative Visualization;Perceptual Evaluation;Hatching;Stippling","Encoding;Image color analysis;Rendering (computer graphics);Data visualization;Surface texture;Electronic mail;Visualization","","1","","55","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco","Z. Zeng; J. Yang; D. Moritz; J. Heer; L. Battle","University of Maryland, College Park, USA; University of Washington, Seattle, USA; Carnegie Mellon University, United States; University of Washington, Seattle, USA; University of Washington, Seattle, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1063","1073","Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco—a framework to model visualization knowledge—to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.","1941-0506","","10.1109/TVCG.2023.3326527","Moore Foundation; NSF(grant numbers:IIS-1850115,IIS-1901386,IIS-2141506); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290999","Graphical Perception Studies;Visualization Recommendation Algorithms","Data visualization;Encoding;Pipelines;Guidelines;Clustering algorithms;Visualization;Surveys","","1","","51","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"A Comparative Study of the Perceptual Sensitivity of Topological Visualizations to Feature Variations","T. M. Athawale; B. Triana; T. Kotha; D. Pugmire; P. Rosen","Oak Ridge National Laboratory, USA; University of South Florida, USA; University of South Florida, USA; Oak Ridge National Laboratory, USA; University of Utah, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","1074","1084","Color maps are a commonly used visualization technique in which data are mapped to optical properties, e.g., color or opacity. Color maps, however, do not explicitly convey structures (e.g., positions and scale of features) within data. Topology-based visualizations reveal and explicitly communicate structures underlying data. Although our understanding of what types of features are captured by topological visualizations is good, our understanding of people's perception of those features is not. This paper evaluates the sensitivity of topology-based isocontour, Reeb graph, and persistence diagram visualizations compared to a reference color map visualization for synthetically generated scalar fields on 2-manifold triangular meshes embedded in 3D. In particular, we built and ran a human-subject study that evaluated the perception of data features characterized by Gaussian signals and measured how effectively each visualization technique portrays variations of data features arising from the position and amplitude variation of a mixture of Gaussians. For positional feature variations, the results showed that only the Reeb graph visualization had high sensitivity. For amplitude feature variations, persistence diagrams and color maps demonstrated the highest sensitivity, whereas isocontours showed only weak sensitivity. These results take an important step toward understanding which topology-based tools are best for various data and task scenarios and their effectiveness in conveying topological variations as compared to conventional color mapping.","1941-0506","","10.1109/TVCG.2023.3326592","U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); National Science Foundation(grant numbers:III-2316496); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292439","Perception & cognition;computational topology-based techniques;comparison and similarity","Data visualization;Image color analysis;Sensitivity;Level set;Visualization;Topology;Task analysis","","","","91","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"ExTreeM: Scalable Augmented Merge Tree Computation via Extremum Graphs","J. Lukasczyk; M. Will; F. Wetzels; G. H. Weber; C. Garth","RPTU Kaiserslautern-Landau, Germany; RPTU Kaiserslautern-Landau, Germany; RPTU Kaiserslautern-Landau, Germany; Lawrence Berkeley National Lab., USA; RPTU Kaiserslautern-Landau, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1085","1094","Over the last decade merge trees have been proven to support a plethora of visualization and analysis tasks since they effectively abstract complex datasets. This paper describes the ExTreeM-Algorithm: A scalable algorithm for the computation of merge trees via extremum graphs. The core idea of ExTreeM is to first derive the extremum graph $\mathcal{G}$ of an input scalar field $f$ defined on a cell complex $\mathcal{K}$, and subsequently compute the unaugmented merge tree of $f$ on $\mathcal{G}$ instead of $\mathcal{K}$; which are equivalent. Any merge tree algorithm can be carried out significantly faster on $\mathcal{G}$, since $\mathcal{K}$ in general contains substantially more cells than $\mathcal{G}$. To further speed up computation, ExTreeM includes a tailored procedure to derive merge trees of extremum graphs. The computation of the fully augmented merge tree, i.e., a merge tree domain segmentation of $\mathcal{K}$, can then be performed in an optional post-processing step. All steps of ExTreeM consist of procedures with high parallel efficiency, and we provide a formal proof of its correctness. Our experiments, performed on publicly available datasets, report a speedup of up to one order of magnitude over the state-of-the-art algorithms included in the TTK and VTK-m software libraries, while also requiring significantly less memory and exhibiting excellent scaling behavior.","1941-0506","","10.1109/TVCG.2023.3326526","Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy Office of Science; National Nuclear Security Administration(grant numbers:DE-AC02-05CH11231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290925","Scalar field topology;merge trees;persistence pairs;high performance computing","Topology;Manifolds;Task analysis;Memory management;Data visualization;Classification algorithms;Data analysis","","","","41","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Merge Tree Geodesics and Barycenters with Path Mappings","F. Wetzels; M. Pont; J. Tierny; C. Garth","University of Kaiserslautern-Landau, Germany; CNRS / Sorbonne University, France; CNRS / Sorbonne University, France; University of Kaiserslautern-Landau, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1095","1105","Comparative visualization of scalar fields is often facilitated using similarity measures such as edit distances. In this paper, we describe a novel approach for similarity analysis of scalar fields that combines two recently introduced techniques: Wasserstein geodesics/barycenters as well as path mappings, a branch decomposition-independent edit distance. Effectively, we are able to leverage the reduced susceptibility of path mappings to small perturbations in the data when compared with the original Wasserstein distance. Our approach therefore exhibits superior performance and quality in typical tasks such as ensemble summarization, ensemble clustering, and temporal reduction of time series, while retaining practically feasible runtimes. Beyond studying theoretical properties of our approach and discussing implementation aspects, we describe a number of case studies that provide empirical insights into its utility for comparative visualization, and demonstrate the advantages of our method in both synthetic and real-world scenarios. We supply a C++ implementation that can be used to reproduce our results.","1941-0506","","10.1109/TVCG.2023.3326601","Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:442077441); European Commission(grant numbers:ERC-2019-COG); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296528","Topological data analysis;merge trees;scalar data;ensemble data","Time series analysis;Data visualization;Level measurement;Task analysis;Layout;Surveys;Interpolation","","","","68","CCBY","25 Oct 2023","","","IEEE","IEEE Journals"
"Data Type Agnostic Visual Sensitivity Analysis","N. Piccolotto; M. Bögl; C. Muehlmann; K. Nordhausen; P. Filzmoser; J. Schmidt; S. Miksch","TU Wien, Austria; TU Wien, Austria; TU Wien, Austria; University of Jyväskylä, Finland; TU Wien, Austria; VRVis GmbH, Austria; TU Wien, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1106","1116","Modern science and industry rely on computational models for simulation, prediction, and data analysis. Spatial blind source separation (SBSS) is a model used to analyze spatial data. Designed explicitly for spatial data analysis, it is superior to popular non-spatial methods, like PCA. However, a challenge to its practical use is setting two complex tuning parameters, which requires parameter space analysis. In this paper, we focus on sensitivity analysis (SA). SBSS parameters and outputs are spatial data, which makes SA difficult as few SA approaches in the literature assume such complex data on both sides of the model. Based on the requirements in our design study with statistics experts, we developed a visual analytics prototype for data type agnostic visual sensitivity analysis that fits SBSS and other contexts. The main advantage of our approach is that it requires only dissimilarity measures for parameter settings and outputs (Fig. 1). We evaluated the prototype heuristically with visualization experts and through interviews with two SBSS experts. In addition, we show the transferability of our approach by applying it to microclimate simulations. Study participants could confirm suspected and known parameter-output relations, find surprising associations, and identify parameter subspaces to examine in the future. During our design study and evaluation, we identified challenging future research opportunities.","1941-0506","","10.1109/TVCG.2023.3327203","Austrian Science Fund (FWF)(grant numbers:P31881-N32); Vienna Science and Technology Fund (WWTF)(grant numbers:10.47379/ICT19047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308617","Visual analytics;parameter space analysis;sensitivity analysis;spatial blind source separation","Data visualization;Task analysis;Spatial databases;Sensitivity analysis;Analytical models;Data models;Predictive models","","","","72","CCBY","3 Nov 2023","","","IEEE","IEEE Journals"
"LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce","Y. Wu; Y. Xu; S. Gao; X. Wang; W. Song; Z. Nie; X. Fan; Q. Li","School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China; School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China; School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China; Weill Cornell Medical College, Cornell University, USA; School of Entrepreneurship and Management, ShanghaiTech University, China; Be Friends Holding Limited, China; School of Entrepreneurship and Management, ShanghaiTech University, China; School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1117","1127","Livestream e-commerce integrates live streaming and online shopping, allowing viewers to make purchases while watching. However, effective marketing strategies remain a challenge due to limited empirical research and subjective biases from the absence of quantitative data. Current tools fail to capture the interdependence between live performances and feedback. This study identified computational features, formulated design requirements, and developed LiveRetro, an interactive visual analytics system. It enables comprehensive retrospective analysis of livestream e-commerce for streamers, viewers, and merchandise. LiveRetro employs enhanced visualization and time-series forecasting models to align performance features and feedback, identifying influences at channel, merchandise, feature, and segment levels. Through case studies and expert interviews, the system provides deep insights into the relationship between live performance and streaming statistics, enabling efficient strategic analysis from multiple perspectives.","1941-0506","","10.1109/TVCG.2023.3326911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295389","Livestream E-commerce;Visual Analytics;Multimodal Video Analysis;Marketing Strategy;Time-series Modeling","Electronic commerce;Streaming media;Visual analytics;Interviews;Forecasting;Behavioral sciences;Analytical models","Retrospective Studies;Computer Graphics;Commerce;Empirical Research;Consumer Behavior","","","79","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Data Formulator: AI-Powered Concept-Driven Visualization Authoring","C. Wang; J. Thompson; B. Lee","Microsoft Research, USA; Microsoft Research, USA; Microsoft Research, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1128","1138","With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.","1941-0506","","10.1109/TVCG.2023.3326585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292609","AI;visualization authoring;data transformation;programming by example;natural language;large language model","Data visualization;Temperature distribution;Urban areas;Visualization;Transforms;Histograms;Libraries","","","","63","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"InvVis: Large-Scale Data Embedding for Invertible Visualization","H. Ye; C. Li; Y. Li; C. Wang","School of Computer Science and Technology, East China Normal University, China; School of Computer Science and Technology, East China Normal University, China; School of Computer Science and Technology, East China Normal University, China; School of Computer Science and Technology, East China Normal University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1139","1149","We present InvVis, a new approach for invertible visualization, which is reconstructing or further modifying a visualization from an image. InvVis allows the embedding of a significant amount of data, such as chart data, chart information, source code, etc., into visualization images. The encoded image is perceptually indistinguishable from the original one. We propose a new method to efficiently express chart data in the form of images, enabling large-capacity data embedding. We also outline a model based on the invertible neural network to achieve high-quality data concealing and revealing. We explore and implement a variety of application scenarios of InvVis. Additionally, we conduct a series of evaluation experiments to assess our method from multiple perspectives, including data embedding quality, data restoration accuracy, data encoding capacity, etc. The result of our experiments demonstrates the great potential of InvVis in invertible visualization.","1941-0506","","10.1109/TVCG.2023.3326597","NSFC(grant numbers:62102152,62072183,61802128); Shanghai Municipal Science and Technology Major Project(grant numbers:22511104600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290968","Information visualization;information steganography;invertible visualization;invertible neural network","Data visualization;Steganography;Image restoration;Visualization;Image reconstruction;Metadata;QR codes","","","","57","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Attribute-Aware RBFs: Interactive Visualization of Time Series Particle Volumes Using RT Core Range Queries","N. Morrical; S. Zellmann; A. Sahistan; P. Shriwise; V. Pascucci","Scientific Computing and Imaging Institute, University of Utah, USA; University of Cologne, USA; Scientific Computing and Imaging Institute, University of Utah, USA; Argonne National Laboratory, USA; Scientific Computing and Imaging Institute, University of Utah, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1150","1160","Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.","1941-0506","","10.1109/TVCG.2023.3327366","UChicago Argonne, LLC; Operator of Argonne National Laboratory (“Argonne”); U.S. Department of Energy Office of Science laboratory(grant numbers:DE-AC02-06CH11357); Department of Energy (DOE)(grant numbers:001425594); Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(grant numbers:456842964); NSF OAC(grant numbers:2138811); NSF CI(grant numbers:2127548,DE-FE0031880); Intel oneAPI Centers of Excellence; University of Utah; Exascale Computing Project(grant numbers:17-SC-20-SC); NNSA; UT-Battelle LLC(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296023","Ray Tracing;Volume Rendering;Particle Volumes;Radial Basis Functions;Scientific Visualization","Ray tracing;Graphics processing units;Rendering (computer graphics);Data visualization;Color;Time series analysis;Memory management","","1","","48","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges","D. Braun; R. Borgo; M. Sondag; T. von Landesberger","University of Cologne, Germany; King's College London, USA; University of Cologne, Germany; University of Cologne, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1161","1171","We introduce two novel visualization designs to support practitioners in performing identification and discrimination tasks on large value ranges (i.e., several orders of magnitude) in time-series data: (1) The order of magnitude horizon graph, which extends the classic horizon graph; and (2) the order of magnitude line chart, which adapts the log-line chart. These new visualization designs visualize large value ranges by explicitly splitting the mantissa $m$ and exponent $e$ of a value $v=m\cdot 10^{e}$. We evaluate our novel designs against the most relevant state-of-the-art visualizations in an empirical user study. It focuses on four main tasks commonly employed in the analysis of time-series and large value ranges visualization: identification, discrimination, estimation, and trend detection. For each task we analyze error, confidence, and response time. The new order of magnitude horizon graph performs better or equal to all other designs in identification, discrimination, and estimation tasks. Only for trend detection tasks, the more traditional horizon graphs reported better performance. Our results are domain-independent, only requiring time-series data with large value ranges.","1941-0506","","10.1109/TVCG.2023.3326576","BMBF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290958","Visualization techniques;time-series;design study;orders of magnitude;logarithmic scale","Data visualization;Task analysis;Image color analysis;Bars;Standards;Market research;Estimation","","","","47","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning","Y. Shi; B. Chen; Y. Chen; Z. Jin; K. Xu; X. Jiao; T. Gao; N. Cao","Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China; Huawei Cloud Computing Technologies Co., Ltd., China; Huawei Cloud Computing Technologies Co., Ltd., China; Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China; Intelligent Big Data Visualization Lab, Tongji University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1172","1182","The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.","1941-0506","","10.1109/TVCG.2023.3327200","NSFC(grant numbers:62061136003,62072338,62002267); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297595","Time Series Data;Exploratory Visual Analysis;Reinforcement Learning","Time series analysis;Data visualization;Task analysis;Data mining;Data analysis;Visual analytics;Reinforcement learning","","1","","77","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations","J. Hao; Q. Shi; Y. Ye; W. Zeng","Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1183","1193","Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.","1941-0506","","10.1109/TVCG.2023.3327389","National Natural Science Foundation of China(grant numbers:62172398); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2021A1515011700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297593","Time-series forecasting;counterfactual explanation;visual analytics","Forecasting;Predictive models;Data visualization;Biological system modeling;Data models;Task analysis;Measurement","","","","67","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Visualizing Large-Scale Spatial Time Series with GeoChron","Z. Deng; S. Chen; T. Schreck; D. Deng; T. Tang; M. Xu; D. Weng; Y. Wu","State Key Lab of CAD&CG, Zhejiang University, China; School of Software Technology, Zhejiang University, China; Graz University of Technology, Austria; School of Software Technology, Zhejiang University, China; School of Art and Archaeology, Zhejiang University, China; School of Computer and Artificial Intelligence, Zhengzhou University, China; School of Software Technology, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1194","1204","In geo-related fields such as urban informatics, atmospheric science, and geography, large-scale spatial time (ST) series (i.e., geo-referred time series) are collected for monitoring and understanding important spatiotemporal phenomena. ST series visualization is an effective means of understanding the data and reviewing spatiotemporal phenomena, which is a prerequisite for in-depth data analysis. However, visualizing these series is challenging due to their large scales, inherent dynamics, and spatiotemporal nature. In this study, we introduce the notion of patterns of evolution in ST series. Each evolution pattern is characterized by 1) a set of ST series that are close in space and 2) a time period when the trends of these ST series are correlated. We then leverage Storyline techniques by considering an analogy between evolution patterns and sessions, and finally design a novel visualization called GeoChron, which is capable of visualizing large-scale ST series in an evolution pattern-aware and narrative-preserving manner. GeoChron includes a mining framework to extract evolution patterns and two-level visualizations to enhance its visual scalability. We evaluate GeoChron with two case studies, an informal user study, an ablation study, parameter analysis, and running time analysis.","1941-0506","","10.1109/TVCG.2023.3327162","National Key R&D Program of China(grant numbers:2022YFE0137800); NSFC(grant numbers:U22A2032); Collaborative Innovation Center of Artificial Intelligence; MOE; Zhejiang Provincial Government (ZJU); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297582","Spatiotemporal visualization;spatial time series;Storyline","Visualization;Market research;Spatiotemporal phenomena;Correlation;Data visualization;Time series analysis;Layout","","","","76","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"HealthPrism: A Visual Analytics System for Exploring Children's Physical and Mental Health Profiles with Multimodal Data","Z. Jiang; H. Chen; R. Zhou; J. Deng; X. Zhang; R. Zhao; C. Xie; Y. Wang; E. C. H. Ngai","University of Hong Kong, China; University of Hong Kong, China; University of Hong Kong, China; University of Hong Kong, China; University of Hong Kong, China; University of Hong Kong, China; Tencent, China; Kellogg School of Management, Northwestern University, USA; University of Hong Kong, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1205","1215","The correlation between children's personal and family characteristics (e.g., demographics and socioeconomic status) and their physical and mental health status has been extensively studied across various research domains, such as public health, medicine, and data science. Such studies can provide insights into the underlying factors affecting children's health and aid in the development of targeted interventions to improve their health outcomes. However, with the availability of multiple data sources, including context data (i.e., the background information of children) and motion data (i.e., sensor data measuring activities of children), new challenges have arisen due to the large-scale, heterogeneous, and multimodal nature of the data. Existing statistical hypothesis-based and learning model-based approaches have been inadequate for comprehensively analyzing the complex correlation between multimodal features and multi-dimensional health outcomes due to the limited information revealed. In this work, we first distill a set of design requirements from multiple levels through conducting a literature review and iteratively interviewing 11 experts from multiple domains (e.g., public health and medicine). Then, we propose HealthPrism, an interactive visual and analytics system for assisting researchers in exploring the importance and influence of various context and motion features on children's health status from multi-level perspectives. Within HealthPrism, a multimodal learning model with a gate mechanism is proposed for health profiling and cross-modality feature importance comparison. A set of visualization components is designed for experts to explore and understand multimodal data freely. We demonstrate the effectiveness and usability of HealthPrism through quantitative evaluation of the model performance, case studies, and expert interviews in associated domains.","1941-0506","","10.1109/TVCG.2023.3326943","GRF(grant numbers:17203320,17209822); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294211","Visual Analytics;Health Profiling;Multimodal Learning;Context Data;Motion Data","Artificial intelligence;TV;Sun;Correlation;Germanium;Visual analytics;Pediatrics","","1","","70","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Marjorie: Visualizing Type 1 Diabetes Data to Support Pattern Exploration","A. Scimone; K. Eckelt; M. Streit; A. Hinterreiter","Johannes Kepler University Linz, Austria; Johannes Kepler University Linz, Austria; Johannes Kepler University Linz, Austria; Johannes Kepler University Linz, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1216","1226","In this work we propose Marjorie, a visual analytics approach to address the challenge of analyzing patients' diabetes data during brief regular appointments with their diabetologists. Designed in consultation with diabetologists, Marjorie uses a combination of visual and algorithmic methods to support the exploration of patterns in the data. Patterns of interest include seasonal variations of the glucose profiles, and non-periodic patterns such as fluctuations around mealtimes or periods of hypoglycemia (i.e., glucose levels below the normal range). We introduce a unique representation of glucose data based on modified horizon graphs and hierarchical clustering of adjacent carbohydrate or insulin entries. Semantic zooming allows the exploration of patterns on different levels of temporal detail. We evaluated our solution in a case study, which demonstrated Marjorie's potential to provide valuable insights into therapy parameters and unfavorable eating habits, among others. The study results and informal feedback collected from target users suggest that Marjorie effectively supports patients and diabetologists in the joint exploration of patterns in diabetes data, potentially enabling more informed treatment decisions. A free copy of this paper and all supplemental materials are available at https://osf.io/34t8c/.","1941-0506","","10.1109/TVCG.2023.3326936","Austrian Research Promotion Agency(grant numbers:FFG 881844); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294207","Design study;task analysis;diabetes;time series data;visual analytics;clustering","Glucose;Insulin;Diabetes;Data visualization;Blood;Time series analysis;Medical treatment","Humans;Diabetes Mellitus, Type 1;Computer Graphics;Insulin;Glucose","","","50","CCBY","24 Oct 2023","","","IEEE","IEEE Journals"
"Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining","C. Floricel; A. Wentzel; A. Mohamed; C. D. Fuller; G. Canahuate; G. E. Marai","University of Illinois Chicago, USA; University of Illinois Chicago, USA; M.D. Anderson Cancer Center at the University of Texas, USA; M.D. Anderson Cancer Center at the University of Texas, USA; University of Iowa, USA; University of Illinois Chicago, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1227","1237","Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.","1941-0506","","10.1109/TVCG.2023.3326939","NIH(grant numbers:NCI-R01-CA258827,NLM-R01-LM012527); NSF(grant numbers:CDSE-1854815,CNS-1828265); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330756","Temporal Data;Life Sciences;Mixed Initiative Human-Machine Analysis;Data Clustering and Aggregation","Visualization;Cancer;Computational modeling;Data visualization;Data mining;Analytical models;Data models","Humans;Rosa;Quality of Life;Computer Graphics;Data Mining","","","82","IEEE","28 Nov 2023","","","IEEE","IEEE Journals"
"Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning","Y. Ouyang; Y. Wu; H. Wang; C. Zhang; F. Cheng; C. Jiang; L. Jin; Y. Cao; Q. Li","School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China; School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China; School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, ETH Zürich, Switzerland; Zhongshan Hospital Fudan University, China; Zhongshan Hospital Fudan University, China; Zhongshan Hospital Fudan University, China; School of Information Science and Technology, ShanghaiTech University, and Shanghai Engineering Research Center of Intelligent Vision and Imaging, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1238","1248","Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant, a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.","1941-0506","","10.1109/TVCG.2023.3326929","National Natural Science Foundation of China(grant numbers:82001471); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295394","Multimodal Medical Dataset;Visual Analytics;Explainable Machine Learning","Medical diagnostic imaging;Solid modeling;Computational modeling;Medical services;Data visualization;Data models;Training","Humans;Computer Graphics;Learning;Feedback;Education, Medical;Medical Records","","","74","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"TROPHY: A Topologically Robust Physics-Informed Tracking Framework for Tropical Cyclones","L. Yan; H. Guo; T. Peterka; B. Wang; J. Wang","Argonne National Laboratory, USA; Ohio State University, USA; Argonne National Laboratory, USA; University of Utah, USA; Argonne National Laboratory, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1249","1259","Tropical cyclones (TCs) are among the most destructive weather systems. Realistically and efficiently detecting and tracking TCs are critical for assessing their impacts and risks. In particular, the eye is a signature feature of a mature TC. Therefore, knowing the eyes' locations and movements is crucial for both operational weather forecasts and climate risk assessments. Recently, a multilevel robustness framework has been introduced to study the critical points of time-varying vector fields. The framework quantifies the robustness (i.e., structural stability) of critical points across varying neighborhoods. By relating the multilevel robustness with critical point tracking, the framework has demonstrated its potential in cyclone tracking. An advantage is that it identifies cyclonic features using only 2D wind vector fields, which is encouraging as most tracking algorithms require multiple dynamic and thermodynamic variables at different altitudes. A disadvantage is that the framework does not scale well computationally for datasets containing a large number of cyclones. This paper introduces a topologically robust physics-informed tracking framework (TROPHY) for TC tracking. The main idea is to integrate physical knowledge of TC to drastically improve the computational efficiency of multilevel robustness framework for large-scale climate datasets. First, during preprocessing, we propose a physics-informed feature selection strategy to filter 90% of critical points that are short-lived and have low stability, thus preserving good candidates for TC tracking. Second, during in-processing, we impose constraints during the multilevel robustness computation to focus only on physics-informed neighborhoods of TCs. We apply TROPHY to 30 years of 2D wind fields from reanalysis data in ERA5 and generate a number of TC tracks. In comparison with the observed tracks, we demonstrate that TROPHY can capture TC characteristics (e.g., frequency, intensity, duration, latitudes with maximum intensity, and genesis) that are comparable to and sometimes even better than a well-validated TC tracking algorithm that requires multiple dynamic and thermodynamic scalar fields.","1941-0506","","10.1109/TVCG.2023.3326905","Wind Energy Technologies Office of the DOE. Argonne National Laboratory is a US Department of Energy laboratory managed by UChicago Argonne, LLC(grant numbers:DE-AC02-06CH11357); DOE(grant numbers:DE-SC0022753,DE-SC0023193,DE-SC0021015); NSF(grant numbers:IIS 2145499,IIS 1910733); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309869","Feature tracking;robustness;topology-based methods in visualization;applications;climate science;tropical cyclones","Robustness;Meteorology;Topology;Heuristic algorithms;Feature extraction;Wind speed;Tropical cyclones","","","","56","IEEE","6 Nov 2023","","","IEEE","IEEE Journals"
"A Local Iterative Approach for the Extraction of 2D Manifolds from Strongly Curved and Folded Thin-Layer Structures","N. Klenert; V. Lepper; D. Baum","Zuse Institute Berlin, Germany; Agyptisches Museum und Papyrussammlung, Germany; Zuse Institute Berlin, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1260","1270","Ridge surfaces represent important features for the analysis of 3-dimensional (3D) datasets in diverse applications and are often derived from varying underlying data including flow fields, geological fault data, and point data, but they can also be present in the original scalar images acquired using a plethora of imaging techniques. Our work is motivated by the analysis of image data acquired using micro-computed tomography ($\mu\text{CT}$) of ancient, rolled and folded thin-layer structures such as papyrus, parchment, and paper as well as silver and lead sheets. From these documents we know that they are 2-dimensional (2D) in nature. Hence, we are particularly interested in reconstructing 2D manifolds that approximate the document's structure. The image data from which we want to reconstruct the 2D manifolds are often very noisy and represent folded, densely-layered structures with many artifacts, such as ruptures or layer splitting and merging. Previous ridge-surface extraction methods fail to extract the desired 2D manifold for such challenging data. We have therefore developed a novel method to extract 2D manifolds. The proposed method uses a local fast marching scheme in combination with a separation of the region covered by fast marching into two sub-regions. The 2D manifold of interest is then extracted as the surface separating the two sub-regions. The local scheme can be applied for both automatic propagation as well as interactive analysis. We demonstrate the applicability and robustness of our method on both artificial data as well as real-world data including folded silver and papyrus sheets.","1941-0506","","10.1109/TVCG.2023.3327403","German Research Foundation (DFG)(grant numbers:BA 5042/2-1,LE 1837/2-1); VolkswagenStiftung (Az 92029) and the European Research Council (Project “ELEPHANTINE”(grant numbers:ID 637692); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309981","Ridge surface;crease surface;2D manifold extraction;fast marching;virtual unfolding;historical documents","Faces;Manifolds;Silver;Data mining;Surface reconstruction;Feature extraction;Topology","","","","42","IEEE","6 Nov 2023","","","IEEE","IEEE Journals"
"A Task-Parallel Approach for Localized Topological Data Structures","G. Liu; F. Iuricich","School of Computing, Clemson University, USA; School of Computing, Clemson University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1271","1281","Unstructured meshes are characterized by data points irregularly distributed in the Euclidian space. Due to the irregular nature of these data, computing connectivity information between the mesh elements requires much more time and memory than on uniformly distributed data. To lower storage costs, dynamic data structures have been proposed. These data structures compute connectivity information on the fly and discard them when no longer needed. However, on-the-fly computation slows down algorithms and results in a negative impact on the time performance. To address this issue, we propose a new task-parallel approach to proactively compute mesh connectivity. Unlike previous approaches implementing data-parallel models, where all threads run the same type of instructions, our task-parallel approach allows threads to run different functions. Specifically, some threads run the algorithm of choice while other threads compute connectivity information before they are actually needed. The approach was implemented in the new Accelerated Clustered TOPOlogical (ACTOPO) data structure, which can support any processing algorithm requiring mesh connectivity information. Our experiments show that ACTOPO combines the benefits of state-of-the-art memory-efficient (TTK CompactTriangulation) and time-efficient (TTK ExplicitTriangulation) topological data structures. It occupies a similar amount of memory as TTK CompactTriangulation while providing up to 5x speedup. Moreover, it achieves comparable time performance as TTK ExplicitTriangulation while using only half of the memory space.","1941-0506","","10.1109/TVCG.2023.3327182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302662","Data structures;parallel computation;topological data analysis;simplicial complex","Data structures;Task analysis;Faces;Memory management;Instruction sets;Computational modeling;Graphics processing units","","","","59","IEEE","31 Oct 2023","","","IEEE","IEEE Journals"
"Global Topology of 3D Symmetric Tensor Fields","S. -H. Hung; Y. Zhang; E. Zhang","School of Electrical Engineering and Computer Science, Oregon State University, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1282","1291","There have been recent advances in the analysis and visualization of 3D symmetric tensor fields, with a focus on the robust extraction of tensor field topology. However, topological features such as degenerate curves and neutral surfaces do not live in isolation. Instead, they intriguingly interact with each other. In this paper, we introduce the notion of topological graph for 3D symmetric tensor fields to facilitate global topological analysis of such fields. The nodes of the graph include degenerate curves and regions bounded by neutral surfaces in the domain. The edges in the graph denote the adjacency information between the regions and degenerate curves. In addition, we observe that a degenerate curve can be a loop and even a knot and that two degenerate curves (whether in the same region or not) can form a link. We provide a definition and theoretical analysis of individual degenerate curves in order to help understand why knots and links may occur. Moreover, we differentiate between wedges and trisectors, thus making the analysis more detailed about degenerate curves. We incorporate this information into the topological graph. Such a graph can not only reveal the global structure in a 3D symmetric tensor field but also allow two symmetric tensor fields to be compared. We demonstrate our approach by applying it to solid mechanics and material science data sets.","1941-0506","","10.1109/TVCG.2023.3326933","NSF(grant numbers:1619383); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294203","Tensor field visualization;3D symmetric tensor fields;global tensor field topology;topological graphs;degenerate curves;neutral surfaces;wedges and trisectors","Tensors;Structural rings;Three-dimensional displays;Topology;Stress;Feature extraction;Data visualization","","","","38","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Interactive Design and Optics-Based Visualization of Arbitrary Non-Euclidean Kaleidoscopic Orbifolds","J. Zheng; E. Zhang; Y. Zhang","Oregon State University, USA; Oregon State University, USA; Oregon State University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1292","1301","Orbifolds are a modern mathematical concept that arises in the research of hyperbolic geometry with applications in computer graphics and visualization. In this paper, we make use of rooms with mirrors as the visual metaphor for orbifolds. Given any arbitrary two-dimensional kaleidoscopic orbifold, we provide an algorithm to construct a Euclidean, spherical, or hyperbolic polygon to match the orbifold. This polygon is then used to create a room for which the polygon serves as the floor and the ceiling. With our system that implements Möbius transformations, the user can interactively edit the scene and see the reflections of the edited objects. To correctly visualize non-Euclidean orbifolds, we adapt the rendering algorithms to account for the geodesics in these spaces, which light rays follow. Our interactive orbifold design system allows the user to create arbitrary two-dimensional kaleidoscopic orbifolds. In addition, our mirror-based orbifold visualization approach has the potential of helping our users gain insight on the orbifold, including its orbifold notation as well as its universal cover, which can also be the spherical space and the hyperbolic space.","1941-0506","","10.1109/TVCG.2023.3326927","NSF(grant numbers:1619383); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294217","Kaleidoscopic Orbifolds;Orbifold Visualization;Math Visualization;Orbifold Construction;Spherical Geometry;Hyperbolic Geometry","Visualization;Mirrors;Rendering (computer graphics);Geometry;Electronic mail;Topology;Deformation","","","","36","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"TopoSZ: Preserving Topology in Error-Bounded Lossy Compression","L. Yan; X. Liang; H. Guo; B. Wang","Argonne National Laboratory, USA; University of Kentucky, USA; Ohio State University, USA; University of Utah, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1302","1312","Existing error-bounded lossy compression techniques control the pointwise error during compression to guarantee the integrity of the decompressed data. However, they typically do not explicitly preserve the topological features in data. When performing post hoc analysis with decompressed data using topological methods, preserving topology in the compression process to obtain topologically consistent and correct scientific insights is desirable. In this paper, we introduce TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. Specifically, we aim to preserve the types and locations of local extrema as well as the level set relations among critical points captured by contour trees in the decompressed data. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain, and incorporate such constraints with a customized error-controlled quantization strategy from the SZ compressor (version 1.4). Our method allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.","1941-0506","","10.1109/TVCG.2023.3326920","DOE(grant numbers:DE-SC0023157,DE-SC0022753,DE-SC0021015); NSF(grant numbers:IIS-1910733,IIS-2145499,OAC-2311878,OAC-2330367); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309863","Lossy compression;contour tree;topology preservation;topological data analysis;topology in visualization","Quantization (signal);Topology;Image coding;Encoding;Level set;Three-dimensional displays;Error correction","","","","68","IEEE","6 Nov 2023","","","IEEE","IEEE Journals"
"ARGUS: Visualization of AI-Assisted Task Guidance in AR","S. Castelo; J. Rulff; E. McGowan; B. Steers; G. Wu; S. Chen; I. Roman; R. Lopez; E. Brewer; C. Zhao; J. Qian; K. Cho; H. He; Q. Sun; H. Vo; J. Bello; M. Krone; C. Silva","New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York; New York University, New York","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1313","1323","The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi-year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant.","1941-0506","","10.1109/TVCG.2023.3327396","DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305427","Data Models;Image and Video Data;Temporal Data;Application Motivated Visualization;AR/VR/Immersive","Task analysis;Data visualization;Data models;Headphones;Real-time systems;Streams;Debugging","","","","58","IEEE","2 Nov 2023","","","IEEE","IEEE Journals"
"Design Patterns for Situated Visualization in Augmented Reality","B. Lee; M. Sedlmair; D. Schmalstieg","University of Stuttgart, Germany; University of Stuttgart, Germany; Graz University of Technology and University of Stuttgart, Austria","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1324","1335","Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.","1941-0506","","10.1109/TVCG.2023.3327398","German Research Foundation (DFG)(grant numbers:495135767); Austria Science Fund (FWF)(grant numbers:I 5912-N); Germany's Excellence Strategy(grant numbers:EXC 2120/1 - 390831618); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297565","Augmented reality;immersive analytics;situated visualization;design patterns;design space","Data visualization;Three-dimensional displays;Surveys;Semantics;Augmented reality;Object recognition;Guidelines","","1","","124","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"Handling Non-Visible Referents in Situated Visualizations","A. Assor; A. Prouzeau; M. Hachet; P. Dragicevic","Inria, CNRS, Université de Bordeaux, France; Inria, CNRS, Université de Bordeaux, France; Inria, CNRS, Université de Bordeaux, France; Inria, CNRS, Université de Bordeaux, France","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1336","1346","Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be beneficial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user's visual field. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We first provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area.","1941-0506","","10.1109/TVCG.2023.3327361","Agence Nationale de la Recherche (ANR)(grant numbers:ANR-19-CE33-0012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296000","Taxonomy;Models;Frameworks;Theory;Mobile;AR/VR/Immersive;Specialized Input/Display Hardware","Data visualization;Three-dimensional displays;Visualization;Terminology;Augmented reality;Task analysis;Surveys","","","","98","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios","Z. Chen; D. Chiappalupi; T. Lin; Y. Yang; J. Beyer; H. Pfister","Harvard John A. Paulson School of Engineering and Applied Sciences, United States; Harvard John A. Paulson School of Engineering and Applied Sciences, United States; Harvard John A. Paulson School of Engineering and Applied Sciences, United States; Virginia Tech, United States; Harvard John A. Paulson School of Engineering and Applied Sciences, United States; Harvard John A. Paulson School of Engineering and Applied Sciences, United States","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1347","1357","Labels are widely used in augmented reality (AR) to display digital information. Ensuring the readability of AR labels requires placing them in an occlusion-free manner while keeping visual links legible, especially when multiple labels exist in the scene. Although existing optimization-based methods, such as force-based methods, are effective in managing AR labels in static scenarios, they often struggle in dynamic scenarios with constantly moving objects. This is due to their focus on generating layouts optimal for the current moment, neglecting future moments and leading to sub-optimal or unstable layouts over time. In this work, we present RL-LABEL, a deep reinforcement learning-based method intended for managing the placement of AR labels in scenarios involving moving objects. RL-LABEL considers both the current and predicted future states of objects and labels, such as positions and velocities, as well as the user's viewpoint, to make informed decisions about label placement. It balances the trade-offs between immediate and long-term objectives. We tested RL-LABEL in simulated AR scenarios on two real-world datasets, showing that it effectively learns the decision-making process for long-term optimization, outperforming two baselines (i.e., no view management and a force-based method) by minimizing label occlusions, line intersections, and label movement distance. Additionally, a user study involving 18 participants indicates that, within our simulated environment, RL-LABEL excels over the baselines in aiding users to identify, compare, and summarize data on labels in dynamic scenes.","1941-0506","","10.1109/TVCG.2023.3326568","NSF(grant numbers:III-2107328,IIS-1901030); NIH(grant numbers:R01HD104969); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290983","Augmented Reality;Reinforcement Learning;Label Placement;Dynamic Scenarios","Three-dimensional displays;Layout;Dynamics;Visualization;Optimization;Task analysis;Sports","","","","58","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"A General Framework for Progressive Data Compression and Retrieval","V. A. P. Magri; P. Lindstrom","Lawrence Livermore National Laboratory., U.S.; Lawrence Livermore National Laboratory., U.S.","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1358","1368","In scientific simulations, observations, and experiments, the transfer of data to and from disk and across networks has become a major bottleneck for data analysis and visualization. Compression techniques have been employed to tackle this challenge, but traditional lossy methods often demand conservative error tolerances to meet the numerical accuracy requirements of both anticipated and unknown data analysis tasks. Progressive data compression and retrieval has emerged as a promising solution, where each analysis task dictates its own accuracy needs. However, few analysis algorithms inherently support progressive data processing, and adapting compression techniques, file formats, client/server frameworks, and APIs to support progressivity can be challenging. This paper presents a framework that enables progressive-precision data queries for any data compressor or numerical representation. Our strategy hinges on a multi-component representation that successively reduces the error between the original and compressed field, allowing each field in the progressive sequence to be expressed as a partial sum of components. We have implemented this approach with four established scientific data compressors and assessed its effectiveness using real-world data sets from the SDRBench collection. The results show that our framework competes in accuracy with the standalone compressors it is based upon. Additionally, (de)compression time is proportional to the number of components requested by the user. Finally, our framework allows for fully lossless compression using lossy compressors when a sufficient number of components are employed.","1941-0506","","10.1109/TVCG.2023.3327186","U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract(grant numbers:DE-AC52-07NA27344); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308629","Lossy to lossless compression;progressive precision;multi-component expansion;floating-point data","Image coding;Costs;Task analysis;Image resolution;Image reconstruction;Encoding;Data analysis","","","","53","IEEE","3 Nov 2023","","","IEEE","IEEE Journals"
"Differentiable Design Galleries: A Differentiable Approach to Explore the Design Space of Transfer Functions","B. Pan; J. Lu; H. Li; W. Chen; Y. Wang; M. Zhu; C. Yu; W. Chen","State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; Zhejiang University of Finance&Economics, China; State Key Lab of CAD&CG, Zhejiang University, China; Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China; State Key Lab of CAD&CG, Zhejiang University, China","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1369","1379","The transfer function is crucial for direct volume rendering (DVR) to create an informative visual representation of volumetric data. However, manually adjusting the transfer function to achieve the desired DVR result can be time-consuming and unintuitive. In this paper, we propose Differentiable Design Galleries, an image-based transfer function design approach to help users explore the design space of transfer functions by taking advantage of the recent advances in deep learning and differentiable rendering. Specifically, we leverage neural rendering to learn a latent design space, which is a continuous manifold representing various types of implicit transfer functions. We further provide a set of interactive tools to support intuitive query, navigation, and modification to obtain the target design, which is represented as a neural-rendered design exemplar. The explicit transfer function can be reconstructed from the target design with a differentiable direct volume renderer. Experimental results on real volumetric data demonstrate the effectiveness of our method.","1941-0506","","10.1109/TVCG.2023.3327371","National Natural Science Foundation of China(grant numbers:U22B2034); Fundamental Research Funds for the Central Universities(grant numbers:226-2022-00235); Natural Science Foundation of Zhejiang Province, China(grant numbers:LY21F020029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296529","Transfer function;direct volume rendering;deep learning;generative models;differentiable rendering","Transfer functions;Space exploration;Rendering (computer graphics);Navigation;Deep learning;Image reconstruction;Generators","","","","50","IEEE","25 Oct 2023","","","IEEE","IEEE Journals"
"Residency Octree: A Hybrid Approach for Scalable Web-Based Multi-Volume Rendering","L. Herzberger; M. Hadwiger; R. Krüger; P. Sorger; H. Pfister; E. Gröller; J. Beyer","TU Wien, Austria; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; John A. Paulson School of Engineering and Applied Sciences at Harvard University, USA; Harvard Medical School, USA; John A. Paulson School of Engineering and Applied Sciences at Harvard University, USA; TU Wien, Austria; John A. Paulson School of Engineering and Applied Sciences at Harvard University, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1380","1390","We present a hybrid multi-volume rendering approach based on a novel Residency Octree that combines the advantages of out-of-core volume rendering using page tables with those of standard octrees. Octree approaches work by performing hierarchical tree traversal. However, in octree volume rendering, tree traversal and the selection of data resolution are intrinsically coupled. This makes fine-grained empty-space skipping costly. Page tables, on the other hand, allow access to any cached brick from any resolution. However, they do not offer a clear and efficient strategy for substituting missing high-resolution data with lower-resolution data. We enable flexible mixed-resolution out-of-core multi-volume rendering by decoupling the cache residency of multi-resolution data from a resolution-independent spatial subdivision determined by the tree. Instead of one-to-one node-to-brick correspondences, each residency octree node is mapped to a set of bricks from different resolution levels. This makes it possible to efficiently and adaptively choose and mix resolutions, adapt sampling rates, and compensate for cache misses. At the same time, residency octrees support fine-grained empty-space skipping, independent of the data subdivision used for caching. Finally, to facilitate collaboration and outreach, and to eliminate local data storage, our implementation is a web-based, pure client-side renderer using WebGPU and WebAssembly. Our method is faster than prior approaches and efficient for many data channels with a flexible and adaptive choice of data resolution.","1941-0506","","10.1109/TVCG.2023.3327193","NCI(grant numbers:U2C-CA233262); NSF(grant numbers:IIS-1901030); BMVIT; BMWFW; SFG; Vienna Business Agency(grant numbers:854174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299573","Volume rendering;ray-guided rendering;large-scale data;out-of-core rendering;multi-resolution;multi-channel;web-based visualization","Rendering (computer graphics);Octrees;Spatial resolution;Graphics processing units;Data visualization;Optimization;Standards","","","","53","IEEE","27 Oct 2023","","","IEEE","IEEE Journals"
"GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation","F. Lei; Y. Ma; A. S. Fotheringham; E. A. Mack; Z. Li; M. Sachdeva; S. Bardin; R. Maciejewski","Arizona State University, USA; Southern University of Science and Technology, China; Arizona State University, USA; Michigan State University, USA; Florida State University, USA; Arizona State University, USA; Arizona State University, USA; Arizona State University, USA","IEEE Transactions on Visualization and Computer Graphics","27 Dec 2023","2024","30","1","1391","1401","Geographic regression models of various descriptions are often applied to identify patterns and anomalies in the determinants of spatially distributed observations. These types of analyses focus on answering why questions about underlying spatial phenomena, e.g., why is crime higher in this locale, why do children in one school district outperform those in another, etc.? Answers to these questions require explanations of the model structure, the choice of parameters, and contextualization of the findings with respect to their geographic context. This is particularly true for local forms of regression models which are focused on the role of locational context in determining human behavior. In this paper, we present GeoExplainer, a visual analytics framework designed to support analysts in creating explanative documentation that summarizes and contextualizes their spatial analyses. As analysts create their spatial models, our framework flags potential issues with model parameter selections, utilizes template-based text generation to summarize model outputs, and links with external knowledge repositories to provide annotations that help to explain the model results. As analysts explore the model results, all visualizations and annotations can be captured in an interactive report generation widget. We demonstrate our framework using a case study modeling the determinants of voting in the 2016 US Presidential Election.","1941-0506","","10.1109/TVCG.2023.3327359","National Science Foundation(grant numbers:1639227); U.S. Department of Homeland Security(grant numbers:2017-ST-061-QA0001); National Natural Science Foundation of China(grant numbers:62202217); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515012889); Guangdong Talent Program(grant numbers:2021QN02X794); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297564","Spatial data analysis;local models;multiscale geographically weighted regression;model explanation;visual analytics","Analytical models;Annotations;Data visualization;Context modeling;Data models;Pipelines;Computational modeling","","","","71","IEEE","26 Oct 2023","","","IEEE","IEEE Journals"
"The Urban Toolkit: A Grammar-Based Framework for Urban Visual Analytics","G. Moreira; M. Hosseini; M. N. Alam Nipu; M. Lage; N. Ferreira; F. Miranda","University of Illinois Chicago, USA; Massachusetts Institute of Technology, USA; University of Illinois Chicago, USA; Universidade Federal Fluminense, Brazil; Universidade Federal de Pernambuco, Brazil; University of Illinois Chicago, USA","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1402","1412","While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their problems, the complex nature of urban issues and the overwhelming amount of available data have posed significant challenges in translating these efforts into actionable insights. In the past few years, urban visual analytics tools have significantly helped tackle these challenges. When analyzing a feature of interest, an urban expert must transform, integrate, and visualize different thematic (e.g., sunlight access, demographic) and physical (e.g., buildings, street networks) data layers, oftentimes across multiple spatial and temporal scales. However, integrating and analyzing these layers require expertise in different fields, increasing development time and effort. This makes the entire visual data exploration and system implementation difficult for programmers and also sets a high entry barrier for urban experts outside of computer science. With this in mind, in this paper, we present the Urban Toolkit (UTK), a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind. In order to facilitate the integration and visualization of different urban data, we also propose the concept of knots to merge thematic and physical urban layers. We evaluate our approach through use cases and a series of interviews with experts and practitioners from different domains, including urban accessibility, urban planning, architecture, and climate science. UTK is available at urbantk.org.","1941-0506","","10.1109/TVCG.2023.3326598","University of Illinois' Discovery Partners Institute (DPI), CNPq(grant numbers:316963/2021-6); FAPERJ(grant numbers:E-26/202.915/2019,E-26/211.134/2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290965","Urban visual analytics;Urban analytics;Urban data;Visualization toolkit","Data visualization;Visual analytics;Grammar;Three-dimensional displays;Buildings;Data models;Stakeholders","","","","74","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"From Shock to Shift: Data Visualization for Constructive Climate Journalism","F. Morini; A. Eschenbacher; J. Hartmann; M. Dörk","Södertörn University and University of Applied Sciences Potsdam, Germany; Filmuniversität Babelsberg, Germany; Filmuniversität Babelsberg, Germany; University of Applied Sciences Potsdam, Germany","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1413","1423","We present a multi-dimensional, multi-level, and multi-channel approach to data visualization for the purpose of constructive climate journalism. Data visualization has assumed a central role in environmental journalism and is often used in data stories to convey the dramatic consequences of climate change and other ecological crises. However, the emphasis on the catastrophic impacts of climate change tends to induce feelings of fear, anxiety, and apathy in readers. Climate mitigation, adaptation, and protection—all highly urgent in the face of the climate crisis—are at risk of being overlooked. These topics are more difficult to communicate as they are hard to convey on varying levels of locality, involve multiple interconnected sectors, and need to be mediated across various channels from the printed newspaper to social media platforms. So far, there has been little research on data visualization to enhance affective engagement with data about climate protection as part of solution-oriented reporting of climate change. With this research we characterize the unique challenges of constructive climate journalism for data visualization and share findings from a research and design study in collaboration with a national newspaper in Germany. Using the affordances and aesthetics of travel postcards, we present Klimakarten, a data journalism project on the progress of climate protection at multiple spatial scales (from national to local), across five key sectors (agriculture, buildings, energy, mobility, and waste), and for print and online use. The findings from quantitative and qualitative analysis of reader feedback confirm our overall approach and suggest implications for future work.","1941-0506","","10.1109/TVCG.2023.3327185","Bundesministerium für Bildung und Forschung(grant numbers:8268110202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308621","Constructive Climate Journalism;Frameworks;Storytelling;Journalism","Data visualization;Meteorology;Journalism;Climate change;Visualization;Complexity theory;Anxiety disorders;Media;Climate change","Humans;Data Visualization;Computer Graphics;Climate Change","","","57","CCBY","3 Nov 2023","","","IEEE","IEEE Journals"
"Embellishments Revisited: Perceptions of Embellished Visualisations Through the Viewer's Lens","M. Alebri; E. Costanza; G. Panagiotidou; D. P. Brumby","University College London and United Arab Emirates University, UAE; University College London, England; King's College London, England; University College London, England","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1424","1434","Embellishments are features commonly used in everyday visualisations which are demonstrated to enhance assimilation and memorability. Despite their popularity, little is known about their impact on enticing readers to explore visualisations. To address this gap, we conducted 18 interviews with a diverse group of participants who were consumers of news media but non-experts in visualisation and design. Participants were shown ten embellished and plain visualisations collected from the news and asked to rank them based on enticement and ease of understanding. Extending prior work, our interview results suggest that visualisations with multiple embellishment types might make a visualisation perceived as more enticing. An important finding from our study is that the widespread of certain embellishments in the media might have made them part of visualisation conventions, making a visualisation appear more objective but less enticing. Based on these findings, we ran a follow-up online user study showing participants variations of the visualisations with multiple embellishments to isolate each embellishment type and investigate its effect. We found that variations with salient embellishments were perceived as more enticing. We argue that to unpack the concept of embellishments; we must consider two factors: embellishment saliency and editorial styles. Our study contributes concept and design considerations to the literature concerned with visualisation design for non-experts in visualisation and design.","1941-0506","","10.1109/TVCG.2023.3326914","United Arab Emirates University and the Engineering and Physical Sciences Research Council BIP project(grant numbers:EP/V042327/1); Ethics Committee of UCLIC(grant numbers:UCLIC_2022_004_Costanza); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10294261","Visualizations;Embellishments;Non-experts in visualisation and design","Visualization;Data visualization;Media;Interviews;Image color analysis;Electronic mail;Bars","","","","47","IEEE","24 Oct 2023","","","IEEE","IEEE Journals"
"Enthusiastic and Grounded, Avoidant and Cautious: Understanding Public Receptivity to Data and Visualizations","H. A. He; J. Walny; S. Thoma; S. Carpendale; W. Willett","University of Calgary, Canada; University of Calgary, Canada; University of Calgary, Canada; Simon Fraser University, Canada; University of Calgary, Canada","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1435","1445","Despite an abundance of open data initiatives aimed to inform and empower “general” audiences, we still know little about the ways people outside of traditional data analysis communities experience and engage with public data and visualizations. To investigate this gap, we present results from an in-depth qualitative interview study with 19 participants from diverse ethnic, occupational, and demographic backgrounds. Our findings characterize a set of lived experiences with open data and visualizations in the domain of energy consumption, production, and transmission. This work exposes information receptivity — an individual's transient state of willingness or openness to receive information —as a blind spot for the data visualization community, complementary to but distinct from previous notions of data visualization literacy and engagement. We observed four clusters of receptivity responses to data- and visualization-based rhetoric: Information-Avoidant, Data-Cautious, Data-Enthusiastic, and Domain-Grounded. Based on our findings, we highlight research opportunities for the visualization community. This exploratory work identifies the existence of diverse receptivity responses, highlighting the need to consider audiences with varying levels of openness to new information. Our findings also suggest new approaches for improving the accessibility and inclusivity of open data and visualization initiatives targeted at broad audiences. A free copy of this paper and all supplemental materials are available at https://OSF.IO/MPQ32.","1941-0506","","10.1109/TVCG.2023.3326917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10292909","Diverse audiences;Information receptivity;Information visualization;Open data","Data visualization;Open data;Interviews;Production;Government;Recruitment;Electronic mail","Humans;Computer Graphics;Data Visualization","","","87","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
"Polarizing Political Polls: How Visualization Design Choices Can Shape Public Opinion and Increase Political Polarization","E. Holder; C. X. Bearfield","3iap, USA; Georgia Tech, Georgia","IEEE Transactions on Visualization and Computer Graphics","25 Dec 2023","2024","30","1","1446","1456","While we typically focus on data visualization as a tool for facilitating cognitive tasks (e.g. learning facts, making decisions), we know relatively little about their second-order impacts on our opinions, attitudes, and values. For example, could design or framing choices interact with viewers' social cognitive biases in ways that promote political polarization? When reporting on U.S. attitudes toward public policies, it is popular to highlight the gap between Democrats and Republicans (e.g. with blue vs red connected dot plots). But these charts may encourage social-normative conformity, influencing viewers' attitudes to match the divided opinions shown in the visualization. We conducted three experiments examining visualization framing in the context of social conformity and polarization. Crowdworkers viewed charts showing simulated polling results for public policy proposals. We varied framing (aggregating data as non-partisan “All US Adults,” or partisan “Democrat” / “Republican”) and the visualized groups' support levels. Participants then reported their own support for each policy. We found that participants' attitudes biased significantly toward the group attitudes shown in the stimuli and this can increase inter-party attitude divergence. These results demonstrate that data visualizations can induce social conformity and accelerate political polarization. Choosing to visualize partisan divisions can divide us further.","1941-0506","","10.1109/TVCG.2023.3326512","NSF(grant numbers:IIS-2237585); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290962","Political Polarization;Public Opinion;Social Categorization;Survey Data;Social Influence;Attitude Change","Data visualization;Public policy;Psychology;Uncertainty;Surveys;Shape;Climate change","Adult;Humans;Public Opinion;Politics;Computer Graphics;Attitude","","","88","IEEE","23 Oct 2023","","","IEEE","IEEE Journals"
